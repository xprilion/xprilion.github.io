1:"$Sreact.fragment"
2:I[6575,["177","static/chunks/app/layout-29f44809ba3fe938.js"],"ThemeProvider"]
3:I[4922,[],""]
4:I[3720,[],""]
6:I[2466,[],"MetadataBoundary"]
8:I[2466,[],"OutletBoundary"]
b:I[6114,[],"AsyncMetadataOutlet"]
d:I[2466,[],"ViewportBoundary"]
f:I[4797,[],""]
:HL["/_next/static/media/22966f4f11fece13-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/3df4cf0b22f61940-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/a481f011d1f4a14b-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/feff4f1fc62fae3c-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/2dba37557e579c7e.css","style"]
:HL["/_next/static/css/3ccc3ade3a1e2b98.css","style"]
:HL["/_next/static/css/f3b8208e8dab82f4.css","style"]
0:{"P":null,"b":"5TgeT3CZF-OhoU1XVum39","p":"","c":["","ollama-models-on-google-cloud-run"],"i":false,"f":[[["",{"children":["(blog)",{"children":[["slug","ollama-models-on-google-cloud-run","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/2dba37557e579c7e.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/3ccc3ade3a1e2b98.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"__variable_6ab973","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","type":"image/png","href":"/apple-icon.png","sizes":"96x96"}],["$","link",null,{"rel":"icon","type":"image/svg+xml","href":"/icon0.svg"}],["$","link",null,{"rel":"shortcut icon","href":"/favicon.ico"}],["$","link",null,{"rel":"apple-touch-icon","sizes":"180x180","href":"/apple-touch-icon.png"}],["$","meta",null,{"name":"apple-mobile-web-app-title","content":"xprilion's blog"}],["$","link",null,{"rel":"manifest","href":"/manifest.json"}],["$","meta",null,{"name":"theme-color","content":"#7b46f6"}],["$","meta",null,{"charSet":"utf-8"}],["$","meta",null,{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta",null,{"itemProp":"name","content":"xprilion's blog"}],["$","meta",null,{"itemProp":"description","content":"Hey, I'm Anubhav Singh. I love building software, mixing stacks and making memes."}],["$","meta",null,{"itemProp":"image","content":"/favicon.png"}],["$","meta",null,{"property":"og:title","content":"xprilion's blog"}],["$","meta",null,{"property":"og:description","content":"Hey, I'm Anubhav Singh. I love building software, mixing stacks and making memes."}],["$","meta",null,{"property":"og:image","content":"/favicon.png"}],["$","meta",null,{"property":"og:type","content":"website"}],["$","meta",null,{"name":"twitter:card","content":"summary_large_image"}],["$","meta",null,{"name":"twitter:site","content":"@xprilion"}],["$","meta",null,{"name":"twitter:creator","content":"@xprilion"}],["$","meta",null,{"name":"twitter:title","content":"xprilion's blog"}],["$","meta",null,{"name":"twitter:description","content":"Hey, I'm Anubhav Singh. I love building software, mixing stacks and making memes."}],["$","meta",null,{"name":"twitter:image","content":"/favicon.png"}]]}],["$","body",null,{"className":"antialiased","children":["$","$L2",null,{"attribute":"class","defaultTheme":"system","enableSystem":true,"disableTransitionOnChange":true,"children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]]}]]}],{"children":["(blog)",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":"$0:f:0:1:1:props:children:1:props:children:1:props:children:props:children:props:notFound:0:1:props:style","children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":"$0:f:0:1:1:props:children:1:props:children:1:props:children:props:children:props:notFound:0:1:props:children:props:children:1:props:style","children":404}],["$","div",null,{"style":"$0:f:0:1:1:props:children:1:props:children:1:props:children:props:children:props:notFound:0:1:props:children:props:children:2:props:style","children":["$","h2",null,{"style":"$0:f:0:1:1:props:children:1:props:children:1:props:children:props:children:props:notFound:0:1:props:children:props:children:2:props:children:props:style","children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","ollama-models-on-google-cloud-run","d"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L5",["$","$L6",null,{"children":"$L7"}],[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/f3b8208e8dab82f4.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$L8",null,{"children":["$L9","$La",["$","$Lb",null,{"promise":"$@c"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","UVFwCAYhKomSdbvWbNKGg",{"children":[["$","$Ld",null,{"children":"$Le"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],null]}],false]],"m":"$undefined","G":["$f","$undefined"],"s":false,"S":true}
10:"$Sreact.suspense"
11:I[6114,[],"AsyncMetadata"]
7:["$","$10",null,{"fallback":null,"children":["$","$L11",null,{"promise":"$@12"}]}]
a:null
e:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
9:null
13:I[1701,["943","static/chunks/943-e7e8665b07d20a9a.js","351","static/chunks/351-1cdf3b1c3e027fe8.js","224","static/chunks/224-26f995270d2801dd.js","356","static/chunks/app/(blog)/%5Bslug%5D/page-ee9e64c2cd7650f1.js"],"default"]
14:I[1999,["943","static/chunks/943-e7e8665b07d20a9a.js","351","static/chunks/351-1cdf3b1c3e027fe8.js","224","static/chunks/224-26f995270d2801dd.js","356","static/chunks/app/(blog)/%5Bslug%5D/page-ee9e64c2cd7650f1.js"],""]
15:I[5224,["943","static/chunks/943-e7e8665b07d20a9a.js","351","static/chunks/351-1cdf3b1c3e027fe8.js","224","static/chunks/224-26f995270d2801dd.js","356","static/chunks/app/(blog)/%5Bslug%5D/page-ee9e64c2cd7650f1.js"],"Image"]
16:I[1106,["943","static/chunks/943-e7e8665b07d20a9a.js","351","static/chunks/351-1cdf3b1c3e027fe8.js","224","static/chunks/224-26f995270d2801dd.js","356","static/chunks/app/(blog)/%5Bslug%5D/page-ee9e64c2cd7650f1.js"],"default"]
18:I[4046,["943","static/chunks/943-e7e8665b07d20a9a.js","351","static/chunks/351-1cdf3b1c3e027fe8.js","224","static/chunks/224-26f995270d2801dd.js","356","static/chunks/app/(blog)/%5Bslug%5D/page-ee9e64c2cd7650f1.js"],""]
17:T105b,<p>This blog is a read-along for the repository <a href="https://github.com/xprilion/ollama-cloud-run/">xprilion/ollama-cloud-run</a> which shows how to deploy various models using the Ollama API on Cloud Run, to run inference using CPU only on a serverless platform - incurring bills only when you use them.</p><p><strong>Ollama</strong> is a framework that makes it easy for developers to prototype apps with open models. It comes with a REST API, and this repository provides Dockerfiles and deployment scripts for each model.</p><p><strong>Google Cloud Run</strong> is a fully managed compute platform that automatically scales your stateless containers. You can run code in any language, and all dependencies are included in a container image, which Google Cloud Run handles deployment and scaling for.</p><p>Ispiration (and gemma2b code) from <a href="https://github.com/wietsevenema/samples/tree/main/run/ollama-gemma">wietsevenema/samples</a>.</p><h2 id="models">Models</h2>
<!--kg-card-begin: html-->
<table>
<thead>
<tr>
<th>Model</th>
<th>Version</th>
<th>Folder Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>codegemma</td>
<td>2b</td>
<td><a href="https://github.com/xprilion/ollama-cloud-run/tree/main/codegemma/2b">codegemma/2b</a></td>
</tr>
<tr>
<td>codegemma</td>
<td>7b</td>
<td><a href="https://github.com/xprilion/ollama-cloud-run/tree/main/codegemma/7b">codegemma/7b</a></td>
</tr>
<tr>
<td>gemma</td>
<td>2b</td>
<td><a href="https://github.com/xprilion/ollama-cloud-run/tree/main/gemma/2b">gemma/2b</a></td>
</tr>
<tr>
<td>gemma</td>
<td>7b</td>
<td><a href="https://github.com/xprilion/ollama-cloud-run/tree/main/gemma/7b">gemma/7b</a></td>
</tr>
<tr>
<td>gemma2</td>
<td>9b</td>
<td><a href="https://github.com/xprilion/ollama-cloud-run/tree/main/gemma2/9b">gemma2/9b</a></td>
</tr>
<tr>
<td>llama3</td>
<td>8b</td>
<td><a href="https://github.com/xprilion/ollama-cloud-run/tree/main/llama3/8b">llama3/8b</a></td>
</tr>
<tr>
<td>llava</td>
<td>7b</td>
<td><a href="https://github.com/xprilion/ollama-cloud-run/tree/main/llava/7b">llava/7b</a></td>
</tr>
<tr>
<td>mistral</td>
<td>7b</td>
<td><a href="https://github.com/xprilion/ollama-cloud-run/tree/main/mistral/7b">mistral/7b</a></td>
</tr>
<tr>
<td>phi3</td>
<td>3.8b</td>
<td><a href="https://github.com/xprilion/ollama-cloud-run/tree/main/phi3/3.8b">phi3/3.8b</a></td>
</tr>
<tr>
<td>qwen2</td>
<td>0.5b</td>
<td><a href="https://github.com/xprilion/ollama-cloud-run/tree/main/qwen2/0.5b">qwen2/0.5b</a></td>
</tr>
</tbody>
</table>
<!--kg-card-end: html-->
<h2 id="usage">Usage</h2><p>To build the container with a specific model included and deploy the Ollama API to a publicly accessible URL on Cloud Run, use the following command from the corresponding model's directory. For example, to deploy <code>gemma:2b</code>:</p><pre><code class="language-sh">bash gemma/2b/deploy.sh
</code></pre><p>Respond to any prompts the command gives you. You might need to enable a few APIs and choose a region to deploy to.</p><p>Building the container takes roughly 3-20 minutes, depending on model size.</p><p>Once the command completes, the deploy command shows the public URL of the service.</p><h2 id="explore-the-api">Explore the API</h2><p>Ask the deployed model a question:</p><pre><code class="language-sh">curl &lt;PUBLIC_URL&gt;/api/generate -d \
 '{ 
    "model": "gemma:2b", 
    "prompt": "Why is the sky blue?" 
  }'
</code></pre><p>The first request to a new instance will take some extra setup time because the model is loaded into memory. Ollama keeps the model in memory for 5 minutes.</p><p>For the full Ollama API, refer to <a href="https://github.com/ollama/ollama/blob/main/docs/api.md">the API docs</a>.</p><h2 id="clean-up">Clean Up</h2><p>To clean up after following this short tutorial, you can do the following:</p><ul><li>In Artifact Registry, find the <code>cloud-run-source-deploy</code> repository and remove the container image used by the Cloud Run service you created.</li><li>In Cloud Run, delete the service you created.</li></ul><h2 id="links">Links</h2><ul><li><a href="https://github.com/ollama/ollama">Ollama</a></li></ul><hr><p>Use for research, exploration, and prototyping.</p>5:["$","$L13",null,{"children":[["$","div",null,{"className":"mb-6","children":[["$","$L14",null,{"href":"/posts","className":"text-blue-600 hover:underline mb-4 block","children":"‚Üê Back to all posts"}],["$","h1",null,{"className":"text-2xl md:text-3xl font-bold mb-2","children":"Ollama Models on Cloud Run"}],["$","p",null,{"className":"text-gray-600 dark:text-gray-200 text-sm ","children":"27 June, 2024"}]]}],["$","$L15",null,{"src":"https://digitalpress.fra1.cdn.digitaloceanspaces.com/4ml9m8u/2025/05/a514362c-99a9-4aad-9678-c012e0c85eed.png","alt":"Ollama Models on Cloud Run","loading":"lazy","className":"w-full h-auto mb-6 rounded-lg object-cover object-center shadow-md shadow-gray-400/30 dark:shadow-gray-700/30","width":1000,"height":1000}],["$","$L16",null,{"html":"$17"}],["$","div",null,{"className":"mt-12 pt-8 border-t","children":[["$","h3",null,{"className":"text-xl font-bold mb-4","children":"Share this post"}],["$","div",null,{"className":"flex gap-4","children":[["$","$L14",null,{"href":"https://twitter.com/intent/tweet?text=Ollama%20Models%20on%20Cloud%20Run&url=https%3A%2F%2Fxprilion.com%2Follama-models-on-google-cloud-run","target":"_blank","rel":"noopener noreferrer","className":"text-blue-600 hover:underline","children":"Share on Twitter"}],["$","$L14",null,{"href":"https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fxprilion.com%2Follama-models-on-google-cloud-run&title=Ollama%20Models%20on%20Cloud%20Run","target":"_blank","rel":"noopener noreferrer","className":"text-blue-600 hover:underline","children":"Share on LinkedIn"}]]}]]}],["$","$L18",null,{"id":"MathJax-script","src":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js","async":true,"strategy":"afterInteractive"}],["$","$L18",null,{"id":"MathJax-config","strategy":"beforeInteractive","dangerouslySetInnerHTML":{"__html":"\n          MathJax = {\n            tex: {\n              inlineMath: [['((', '))']],\n              displayMath: [['[[', ']]']],\n            },\n            svg: {\n              fontCache: 'global'\n            }\n          };\n        "}}]]}]
12:{"metadata":[["$","title","0",{"children":"Ollama Models on Cloud Run"}],["$","meta","1",{"name":"description","content":"This blog is a read-along for the repository xprilion/ollama-cloud-run which shows how to deploy various models using the Ollama API on Cloud Run, to run inference using CPU only on a serverless platform - incurring bills only when you use them.\n\nOllama is a framework that makes it easy for developers to prototype apps with open models. It comes with a REST API, and this repository provides Dockerfiles and deployment scripts for each model.\n\nGoogle Cloud Run is a fully managed compute platform t"}],["$","meta","2",{"property":"og:title","content":"Ollama Models on Cloud Run"}],["$","meta","3",{"property":"og:description","content":"This blog is a read-along for the repository xprilion/ollama-cloud-run which shows how to deploy various models using the Ollama API on Cloud Run, to run inference using CPU only on a serverless platform - incurring bills only when you use them.\n\nOllama is a framework that makes it easy for developers to prototype apps with open models. It comes with a REST API, and this repository provides Dockerfiles and deployment scripts for each model.\n\nGoogle Cloud Run is a fully managed compute platform t"}],["$","meta","4",{"property":"og:image","content":"https://digitalpress.fra1.cdn.digitaloceanspaces.com/4ml9m8u/2025/05/a514362c-99a9-4aad-9678-c012e0c85eed.png"}],["$","meta","5",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","6",{"name":"twitter:title","content":"Ollama Models on Cloud Run"}],["$","meta","7",{"name":"twitter:description","content":"This blog is a read-along for the repository xprilion/ollama-cloud-run which shows how to deploy various models using the Ollama API on Cloud Run, to run inference using CPU only on a serverless platform - incurring bills only when you use them.\n\nOllama is a framework that makes it easy for developers to prototype apps with open models. It comes with a REST API, and this repository provides Dockerfiles and deployment scripts for each model.\n\nGoogle Cloud Run is a fully managed compute platform t"}],["$","meta","8",{"name":"twitter:image","content":"https://digitalpress.fra1.cdn.digitaloceanspaces.com/4ml9m8u/2025/05/a514362c-99a9-4aad-9678-c012e0c85eed.png"}]],"error":null,"digest":"$undefined"}
c:{"metadata":"$12:metadata","error":null,"digest":"$undefined"}
