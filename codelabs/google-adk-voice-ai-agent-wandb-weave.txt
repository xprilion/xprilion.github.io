1:"$Sreact.fragment"
2:I[6575,["177","static/chunks/app/layout-03b28790fff4d2e4.js"],"ThemeProvider"]
3:I[4922,[],""]
4:I[3720,[],""]
5:I[3369,["177","static/chunks/app/layout-03b28790fff4d2e4.js"],"GoogleAnalytics"]
7:I[2466,[],"MetadataBoundary"]
9:I[2466,[],"OutletBoundary"]
c:I[6114,[],"AsyncMetadataOutlet"]
e:I[2466,[],"ViewportBoundary"]
10:I[4797,[],""]
:HL["/_next/static/media/22966f4f11fece13-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/3df4cf0b22f61940-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/a481f011d1f4a14b-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/media/feff4f1fc62fae3c-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/5d465971106e06ba.css","style"]
:HL["/_next/static/css/2dba37557e579c7e.css","style"]
:HL["/_next/static/css/f3b8208e8dab82f4.css","style"]
0:{"P":null,"b":"fx8c_MglUFYEsiAz-v7h6","p":"","c":["","codelabs","google-adk-voice-ai-agent-wandb-weave"],"i":false,"f":[[["",{"children":["codelabs",{"children":[["slug","google-adk-voice-ai-agent-wandb-weave","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/5d465971106e06ba.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/2dba37557e579c7e.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"__variable_6ab973","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","type":"image/png","href":"/apple-icon.png","sizes":"96x96"}],["$","link",null,{"rel":"icon","type":"image/svg+xml","href":"/icon0.svg"}],["$","link",null,{"rel":"shortcut icon","href":"/favicon.ico"}],["$","link",null,{"rel":"apple-touch-icon","sizes":"180x180","href":"/apple-touch-icon.png"}],["$","meta",null,{"name":"apple-mobile-web-app-title","content":"xprilion's blog"}],["$","link",null,{"rel":"manifest","href":"/manifest.json"}],["$","meta",null,{"name":"theme-color","content":"#7b46f6"}],["$","meta",null,{"charSet":"utf-8"}],["$","meta",null,{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta",null,{"itemProp":"name","content":"xprilion's blog"}],["$","meta",null,{"itemProp":"description","content":"Hey, I'm Anubhav Singh. I love building software, mixing stacks and making memes."}],["$","meta",null,{"itemProp":"image","content":"/favicon.png"}],["$","meta",null,{"property":"og:title","content":"xprilion's blog"}],["$","meta",null,{"property":"og:description","content":"Hey, I'm Anubhav Singh. I love building software, mixing stacks and making memes."}],["$","meta",null,{"property":"og:image","content":"/favicon.png"}],["$","meta",null,{"property":"og:type","content":"website"}],["$","meta",null,{"name":"twitter:card","content":"summary_large_image"}],["$","meta",null,{"name":"twitter:site","content":"@xprilion"}],["$","meta",null,{"name":"twitter:creator","content":"@xprilion"}],["$","meta",null,{"name":"twitter:title","content":"xprilion's blog"}],["$","meta",null,{"name":"twitter:description","content":"Hey, I'm Anubhav Singh. I love building software, mixing stacks and making memes."}],["$","meta",null,{"name":"twitter:image","content":"/favicon.png"}]]}],["$","body",null,{"className":"antialiased","children":["$","$L2",null,{"attribute":"class","defaultTheme":"system","enableSystem":true,"disableTransitionOnChange":true,"children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}],["$","$L5",null,{"gaId":"G-NKJL65V2YL"}]]}]]}],{"children":["codelabs",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":[["slug","google-adk-voice-ai-agent-wandb-weave","d"],["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":["$L6",["$","$L7",null,{"children":"$L8"}],[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/f3b8208e8dab82f4.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$L9",null,{"children":["$La","$Lb",["$","$Lc",null,{"promise":"$@d"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","bvZPUYMxNhyQKWFz9vr1Z",{"children":[["$","$Le",null,{"children":"$Lf"}],["$","meta",null,{"name":"next-size-adjust","content":""}]]}],null]}],false]],"m":"$undefined","G":["$10","$undefined"],"s":false,"S":true}
11:"$Sreact.suspense"
12:I[6114,[],"AsyncMetadata"]
8:["$","$11",null,{"fallback":null,"children":["$","$L12",null,{"promise":"$@13"}]}]
b:null
f:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
a:null
14:I[5378,["943","static/chunks/943-eb5c9570e2ea5452.js","981","static/chunks/981-982ff48b33ef9837.js","585","static/chunks/585-ef9a120e1d77e4af.js","182","static/chunks/app/codelabs/%5Bslug%5D/page-696caa01994dec0e.js"],"default"]
15:I[4518,["943","static/chunks/943-eb5c9570e2ea5452.js","981","static/chunks/981-982ff48b33ef9837.js","585","static/chunks/585-ef9a120e1d77e4af.js","182","static/chunks/app/codelabs/%5Bslug%5D/page-696caa01994dec0e.js"],"default"]
17:I[1999,["943","static/chunks/943-eb5c9570e2ea5452.js","981","static/chunks/981-982ff48b33ef9837.js","585","static/chunks/585-ef9a120e1d77e4af.js","182","static/chunks/app/codelabs/%5Bslug%5D/page-696caa01994dec0e.js"],""]
18:I[4046,["943","static/chunks/943-eb5c9570e2ea5452.js","981","static/chunks/981-982ff48b33ef9837.js","585","static/chunks/585-ef9a120e1d77e4af.js","182","static/chunks/app/codelabs/%5Bslug%5D/page-696caa01994dec0e.js"],""]
16:T4135,<h2 id="overview">Overview</h2><p>Duration: 1</p><p>Hey there! This codelab walks you through building a voice AI agent end-to-end with Google's Agent Development Kit (ADK). Also, the repository introduces you to Weights &amp; Biases Weave and Weave Inference.</p><h2 id="requirements">Requirements</h2><p>Duration: 2</p><p>In order to follow this codelab, you'll need the following:</p><ol><li>A <a href="https://aistudio.google.com/apikey" rel="noreferrer">Google Gemini API Key</a>.</li><li>A <a href="https://wandb.ai/authorize" rel="noreferrer">Weights &amp; Biases API Key</a>.</li><li>(Optional) <a href="https://docs.astral.sh/uv/" rel="noreferrer">Astral UV</a> installed locally, to run pip installs faster. If not, just remove <code>uv</code> from the <code>uv pip</code> commands.</li><li>(Optional) Nodejs installed locally, to run the UI.</li></ol><h2 id="setup">Setup</h2><p>Duration: 5</p><p>Let's setup our repository for the day.</p><p>We'll be working with this repository - <a href="https://github.com/wandb/voice-ai-agent-workshop">https://github.com/wandb/voice-ai-agent-workshop</a></p><p>Clone it using:</p><pre><code class="language-bash">git clone git@github.com:wandb/voice-ai-agent-workshop.git</code></pre><p>There are 3 folders inside this repo - </p><ol><li>ui</li><li>api</li><li>hello-world</li></ol><p>Let's begin with the <code>hello-world</code> folder!</p><h2 id="hello-world">Hello World</h2><p>Duration: 10</p><p>Let's get inside this folder with:</p><pre><code class="language-bash">cd hello-world</code></pre><p>The remaining commands and instructions for this section are within this folder.</p><p>The two important files in this folder are - </p><ol><li>main.py</li><li>chat_agent/agent.py</li></ol><h3 id="mainpy">main.py</h3><p>This file is mainly a setup of a Google ADK supported FastAPI server. You can observe that it starts a FastAPI server at localhost with port 8080.</p><h3 id="chatagentagentpy">chat_agent/agent.py</h3><p>This file is the meat of this folder. Let's break it down.</p><pre><code class="language-python">import base64
import os
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk import trace as trace_sdk
from opentelemetry.sdk.trace.export import SimpleSpanProcessor
from opentelemetry import trace

from google.adk.agents import LlmAgent
from google.adk.models.lite_llm import LiteLlm
from dotenv import load_dotenv

load_dotenv()
</code></pre><p>These lines just import the required packages for this script.</p><p>Notice that, we import the following - OpenTelemetry, Google ADK and a helper for working with <code>.env</code> files.</p><p>The next block of code deals with configuration constants for OpenTelemetry connection to W&amp;B Weave.</p><pre><code class="language-python"># Configure Weave endpoint and authentication
WANDB_BASE_URL = "https://trace.wandb.ai"
PROJECT_ID = "wandb/hydpy-agent-test"  # e.g., "myteam/myproject"
OTEL_EXPORTER_OTLP_ENDPOINT = f"{WANDB_BASE_URL}/otel/v1/traces"

# Set up authentication
WANDB_API_KEY = os.getenv("WANDB_API_KEY")
AUTH = base64.b64encode(f"api:{WANDB_API_KEY}".encode()).decode()</code></pre><p>Then, we setup the usage of those constants</p><pre><code class="language-python">OTEL_EXPORTER_OTLP_HEADERS = {
    "Authorization": f"Basic {AUTH}",
    "project_id": PROJECT_ID,
}

# Create the OTLP span exporter with endpoint and headers
exporter = OTLPSpanExporter(
    endpoint=OTEL_EXPORTER_OTLP_ENDPOINT,
    headers=OTEL_EXPORTER_OTLP_HEADERS,
)</code></pre><p>Atfer that, we setup the code to use the configuration</p><pre><code class="language-python"># Create a tracer provider and add the exporter
tracer_provider = trace_sdk.TracerProvider()
tracer_provider.add_span_processor(SimpleSpanProcessor(exporter))

# Set the global tracer provider BEFORE importing/using ADK
trace.set_tracer_provider(tracer_provider)</code></pre><p>And finally, we create the ADK Agent in this block</p><pre><code class="language-python"># Create an LLM agent
root_agent = LlmAgent(
    name="chatai_agent",
    model=LiteLlm(model="openai/openai/gpt-oss-20b"),
    instruction=(
        "You are a helpful agent who can chat with the user."
    ),
    tools=[],
)</code></pre><p>Notice the <code>LiteLlm(model="openai/openai/gpt-oss-20b")</code> line. This line instructs the ADK agent to use a custom LiteLLM OpenAI compatible endpoint with model name <code>openai/gpt-oss-20b</code>. You can explore the full list of available models on Weave Inference here - <a href="https://wandb.ai/inference">https://wandb.ai/inference</a></p><p>We're now ready to run the agent!</p><p>On a terminal, let's setup to run this file. First, we install the required packages.</p><pre><code class="language-bash">uv venv
source .venv/bin/activate
uv pip install -r requirements.txt</code></pre><p>But before we run the agent, we need to setup the environment file at <code>.env</code>: </p><pre><code class="language-text"># in .env file
OPENAI_API_BASE=https://api.inference.wandb.ai/v1
OPENAI_API_KEY="YOUR_WANDB_KEY_HERE"

WANDB_API_KEY="YOUR_WANDB_KEY_HERE"</code></pre><p>You could choose to modify the code to avoid the duplicated <code>YOUR_WANDB_KEY_HERE</code> setup, but we set it twice for sake of clarity, in case you jump ship later. </p><p>And then the command to bring up the dev ui of Google ADK</p><pre><code class="language-bash">adk web</code></pre><p>This should bring up the following UI at <code>http://localhost:8000</code>: </p><figure class="kg-card kg-image-card"><img src="https://digitalpress.fra1.cdn.digitaloceanspaces.com/4ml9m8u/2025/09/Screenshot-2025-09-20-at-10.49.24---AM.png" class="kg-image" alt="" loading="lazy" width="2940" height="1674"></figure><p>You can start chatting with the agent right away!</p><h2 id="voice-ai-agent-api">Voice AI Agent API</h2><p>Duration: 10</p><p>Now, let's take a look at the <code>api</code> folder in our repo root.</p><pre><code class="language-bash">cd api</code></pre><p>Here, the packages installation and env setup are very similar. Run the following:</p><pre><code class="language-bash">uv venv
source .venv/bin/activate
uv pip install -r requirements.txt</code></pre><p>However, in this example, we're using Google's Gemini as the model. Let's set the <code>.env</code> file:</p><pre><code class="language-text"># in .env file:
GEMINI_API_KEY="YOUR_GEMINI_API_KEY_HERE"
GOOGLE_GENAI_USE_VERTEXAI=false</code></pre><p>Before we jump into the files, let's run this server.</p><pre><code class="language-bash">uvicorn main:app --reload</code></pre><p>This will make the server start listening for connections at <code>http://localhost:8000</code>. </p><p>If you do not wish to understand what's going on inside these files, you can skip to the next section here. For the brave of heart, onwards!</p><p>There are mainly 2 files of importance in this folder.</p><ol><li>main.py</li><li>google_search_agent/agent.py </li></ol><p>Let's dive into them!</p><h3 id="mainpy-1">main.py</h3><p>This file is critical, and will often be the one you're editing. </p><p>This file is essentially a <strong>FastAPI-based server</strong> that connects clients (like browsers or apps) to a <strong>Gemini-powered agent</strong> through <strong>streaming communication</strong>. It lets a client open a <strong>server-sent events (SSE)</strong> channel to receive responses (text/audio) from the agent in real time, and also provides an endpoint to send messages back to the agent. Let’s go through it block by block.</p><h3 id="imports">Imports</h3><pre><code class="language-python">import os, json, base64, warnings
from pathlib import Path
from dotenv import load_dotenv
</code></pre><ul><li>Standard modules for filesystem, encoding, warnings.</li><li><code>load_dotenv()</code> reads <code>.env</code> files so we can load API keys into environment variables.</li></ul><pre><code class="language-python">from google.genai.types import (Part, Content, Blob)
from google.adk.runners import InMemoryRunner
from google.adk.agents import LiveRequestQueue
from google.adk.agents.run_config import RunConfig
from google.genai import types
</code></pre><ul><li>These are from Google’s <strong>ADK (Agent Development Kit)</strong> and <strong>GenAI</strong> SDK.<ul><li><code>Part</code> and <code>Content</code>: Represent pieces of a conversation (text, audio, etc.).</li><li><code>Blob</code>: For raw binary data (e.g., audio PCM).</li><li><code>InMemoryRunner</code>: Runs an agent in memory (no persistence).</li><li><code>LiveRequestQueue</code>: Manages a queue of incoming client messages for a live session.</li><li><code>RunConfig</code>: Configures the run (text vs audio).</li></ul></li></ul><pre><code class="language-python">from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
</code></pre><ul><li>Sets up a <strong>FastAPI app</strong> with CORS enabled, JSON endpoints, and streaming responses.</li></ul><pre><code class="language-python">from google_search_agent.agent import root_agent
</code></pre><ul><li>Imports a prebuilt agent definition called <code>root_agent</code>. This is the AI brain.</li></ul><pre><code class="language-python">warnings.filterwarnings("ignore", category=UserWarning, module="pydantic")
</code></pre><ul><li>Suppresses pydantic warnings to avoid console noise.</li></ul><hr><h3 id="config-and-app-name">Config and App Name</h3><pre><code class="language-python">load_dotenv()
APP_NAME = "ADK Streaming example"
</code></pre><ul><li>Loads Gemini API key from <code>.env</code>.</li><li>Defines a name for this app.</li></ul><hr><h3 id="function-startagentsession">Function: start_agent_session</h3><pre><code class="language-python">async def start_agent_session(user_id, is_audio=False):
</code></pre><ul><li>Starts a new agent session for a user.</li></ul><p>Inside:</p><ul><li>Creates a runner (<code>InMemoryRunner</code>) with the agent.</li><li>Starts a session tied to that user ID.</li><li>Chooses the modality: <code>"TEXT"</code> or <code>"AUDIO"</code>.</li><li>Configures it with <code>RunConfig</code>.</li><li>Creates a <code>LiveRequestQueue</code> for streaming messages from client → agent.</li><li>Runs the agent live (<code>runner.run_live</code>) which returns:<ul><li><code>live_events</code>: async stream of events from agent.</li><li><code>live_request_queue</code>: the handle to send new client messages.</li></ul></li></ul><p>This function returns both, so the server can bridge them to the client.</p><hr><h3 id="function-agenttoclientsse">Function: agent_to_client_sse</h3><pre><code class="language-python">async def agent_to_client_sse(live_events):
</code></pre><ul><li>Translates agent events into <strong>Server-Sent Events (SSE)</strong> for the client.</li></ul><p>Inside loop:</p><ol><li>If the event is <strong>turn_complete</strong> or <strong>interrupted</strong>, send a simple status message.</li><li>Otherwise, extract the first <code>Part</code> of the content:<ul><li>If it’s audio (<code>audio/pcm</code>), base64 encode the raw audio and stream it out.</li><li>If it’s partial text, send it chunk by chunk.</li></ul></li></ol><p>This ensures the client gets <strong>real-time streaming</strong> of text or audio.</p><hr><h3 id="fastapi-app-setup">FastAPI App Setup</h3><pre><code class="language-python">app = FastAPI()
app.add_middleware(CORSMiddleware, allow_origins=["*"], ...)
active_sessions = {}
</code></pre><ul><li>Creates FastAPI app, enables CORS for all origins.</li><li>Maintains <code>active_sessions</code> dict: maps <code>user_id</code> → <code>LiveRequestQueue</code>.</li></ul><hr><h3 id="health-check-endpoint">Health Check Endpoint</h3><pre><code class="language-python">@app.get("/")
async def root():
    return JSONResponse({"status": "ok", "name": APP_NAME})
</code></pre><ul><li>Simple API health/status check.</li></ul><hr><h3 id="sse-endpoint">SSE Endpoint</h3><pre><code class="language-python">@app.get("/events/{user_id}")
async def sse_endpoint(user_id: int, is_audio: str = "false"):
</code></pre><ul><li>Opens an SSE connection for a client.</li></ul><p>Steps:</p><ol><li>Calls <code>start_agent_session</code> to get <code>live_events</code> + <code>live_request_queue</code>.</li><li>Stores queue in <code>active_sessions</code>.</li><li>Defines cleanup to remove session when client disconnects.</li><li>Defines <code>event_generator</code> which forwards events from <code>agent_to_client_sse</code>.</li><li>Returns a <strong>StreamingResponse</strong> with <code>text/event-stream</code> headers → SSE.</li></ol><p>This is how the agent’s responses stream into the browser.</p><hr><h3 id="send-message-endpoint">Send Message Endpoint</h3><pre><code class="language-python">@app.post("/send/{user_id}")
async def send_message_endpoint(user_id: int, request: Request):
</code></pre><ul><li>Lets client send text/audio back to the agent.</li></ul><p>Steps:</p><ol><li>Looks up the session’s <code>LiveRequestQueue</code> from <code>active_sessions</code>.</li><li>Reads incoming JSON: <code>{ mime_type, data }</code>.</li><li>If it’s text → wrap in <code>Content</code> + <code>Part</code> → send to agent.</li><li>If it’s audio PCM → decode base64 → send as <code>Blob</code>.</li><li>Otherwise → error if unsupported mime type.</li></ol><p>This makes it a <strong>two-way bridge</strong>:</p><ul><li><code>/events/{user_id}</code> streams <strong>agent → client</strong>.</li><li><code>/send/{user_id}</code> sends <strong>client → agent</strong>.</li></ul><h3 id="googlesearchagentagentpy">google_search_agent/agent.py</h3><p>In this file we're simply setting up the Google ADK agent, and you'll be familiar with how this goes if you studied the <code>hello-world</code> section above.</p><p>In case you did, it defines a Google ADK agent to run with name <code>google_search_agent</code> and uses Google's Gemini API. It has a <code>google_search</code> tool with it, so it can provide latest answers by searching the web.</p><h2 id="ui">UI</h2><p>Duration: 5</p><p>Now, we're good to run the UI. On a new terminal (leave the <code>api</code> server running) switch to this folder:</p><pre><code class="language-bash">cd ui</code></pre><p>Now, considering you have NodeJS and <code>pnpm</code> installed, run the following</p><pre><code class="language-bash">pnpm i
pnpm dev</code></pre><p>This will spin up the UI on <code>http://localhost:3000</code>. </p><p>You should see a web app like this:</p><figure class="kg-card kg-image-card"><img src="https://digitalpress.fra1.cdn.digitaloceanspaces.com/4ml9m8u/2025/09/Screenshot-2025-09-20-at-11.05.12---AM.png" class="kg-image" alt="" loading="lazy" width="2940" height="1674"></figure><p>In case your connection state is not in the green, you may need to set the correct URL by clicking gear icon (settings) on the top right. </p><p>If it still doesn't work, your server might have error logs.</p><p>Click the <code>Start Audio</code> button to start talking to the voice AI agent - that can perform Google Search for you, so maybe ask about the chess match played 2 days ago!</p><h2 id="conclusion">Conclusion</h2><p>Duration: 2</p><p>Congratulations on making it here! 🎉</p><p>How easy was that!? Its just a few commands with a lot of defaults to build voice agents with Google ADK! On top of that, you got an introduction to using ADK with Weave Inference, that provides you free credits every month to keep playing!</p><h2 id="whats-next">What's Next?</h2><p>Duration: 1</p><p>You can explore the entire codebase here: </p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/wandb/voice-ai-agent-workshop"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - wandb/voice-ai-agent-workshop</div><div class="kg-bookmark-description">Contribute to wandb/voice-ai-agent-workshop development by creating an account on GitHub.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://digitalpress.fra1.cdn.digitaloceanspaces.com/4ml9m8u/icon/pinned-octocat-093da3e6fa40.svg" alt=""><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">wandb</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://digitalpress.fra1.cdn.digitaloceanspaces.com/4ml9m8u/thumbnail/voice-ai-agent-workshop" alt="" onerror="this.style.display = 'none'"></div></a></figure><p>If you found this codelab fun and useful, share it with someone who can benefit from this!</p><p>Further, read more about Weights &amp; Biases here - https://wandb.ai. </p>6:["$","div",null,{"className":"flex flex-col min-h-screen px-2 md:px-4 bg-background text-foreground","children":[["$","$L14",null,{"title":"Google ADK Voice AI Agent","closeBtn":true}],["$","main",null,{"className":"flex-grow w-full px-1","children":["$","div",null,{"className":"w-full py-2","children":["$","$L15",null,{"htmlContent":"$16","upperContent":"$undefined","bottomContent":"$undefined"}]}]}],["$","div",null,{"className":"w-full mx-auto py-6 max-w-6xl px-4 border-t border-border","children":["$","div",null,{"className":"flex flex-col sm:flex-row justify-between gap-2","children":[["$","div",null,{"className":"text-center sm:text-left text-foreground","children":["© Anubhav Singh ",2025]}],["$","div",null,{"className":"text-center sm:text-right","children":[["$","$L17",null,{"href":"/sitemap.xml","className":"text-foreground hover:text-blue-600 dark:hover:text-primary","children":"Sitemap"}],["$","span",null,{"className":"mx-1 text-foreground","children":"·"}],["$","$L17",null,{"href":"/newsletter","className":"text-foreground hover:text-blue-600 dark:hover:text-primary","children":"Newsletter"}]]}]]}]}],["$","$L18",null,{"id":"MathJax-script","src":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js","async":true}],["$","$L18",null,{"id":"MathJax-config","dangerouslySetInnerHTML":{"__html":"\n            MathJax = {\n              tex: {\n                inlineMath: [['((', '))']],\n                displayMath: [['[[', ']]']],\n              },\n              svg: {\n                fontCache: 'global'\n              }\n            };\n          "}}]]}]
13:{"metadata":[["$","title","0",{"children":"Google ADK Voice AI Agent | xprilion's blog"}],["$","meta","1",{"name":"description","content":"Let's build a Bidi streaming voice AI agent with Google ADK"}],["$","link","2",{"rel":"icon","href":"/logo.jpg"}]],"error":null,"digest":"$undefined"}
d:{"metadata":"$13:metadata","error":null,"digest":"$undefined"}
