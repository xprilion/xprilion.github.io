{"componentChunkName":"component---src-templates-blog-index-tsx","path":"/posts/","result":{"pageContext":{"posts":[{"node":{"id":"65a61d32ffd74b5e496c19eb","slug":"the-story-of-the-wright-brothers-and-samuel-pierpont-langley","url":"https://xprilion.com/the-story-of-the-wright-brothers-and-samuel-pierpont-langley","title":"The Story of the Wright Brothers and Samuel Pierpont Langley","subtitle":null,"brief":"In the dawn of the 20th century, the quest to conquer the skies was akin to a modern space race. The central figures in this drama were the Wright brothers, Orville and Wilbur, and a distinguished figure, Samuel Pierpont Langley.\nIn the quiet town of...","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1705385212421/ca369805-60d4-47d6-8e4e-a1ea4e1cf103.png"},"content":{"html":"<p><em>In the dawn of the 20th century, the quest to conquer the skies was akin to a modern space race. The central figures in this drama were the Wright brothers, Orville and Wilbur, and a distinguished figure, Samuel Pierpont Langley.</em></p>\n<p>In the quiet town of Dayton, Ohio, two brothers, Wilbur and Orville Wright, embarked on a journey that would reshape history. Far from the academic halls and scientific societies, these self-taught engineers, running a modest bicycle shop, began their quest to unlock the secrets of flight. Unlike Samuel Pierpont Langley, who had access to the best resources and the brightest minds, the Wright brothers relied on their intuition, mechanical skills, and unyielding determination.</p>\n<blockquote>\n<p>Wilbur once said, \"I have been afflicted with the belief that flight is possible to man.\"</p>\n</blockquote>\n<p>Their journey was fueled by this belief and a profound understanding of the principles of aerodynamics, which they studied intensely. The brothers built and tested numerous gliders, learning from each failure, steadily moving towards their groundbreaking invention. Their approach was meticulous and grounded in empirical evidence, a stark contrast to the more theoretical approaches of their contemporaries.</p>\n<p>Samuel Pierpont Langley, an esteemed figure in the scientific community, was the antithesis of the Wright brothers. As the Secretary of the Smithsonian Institution and a professor of astronomy, Langley had the backing of the American government, with a grant of $50,000 (a substantial sum at the time) and $20,000 from the Smithsonian to develop a piloted airplane. His project, named the Aerodrome, was a symbol of American scientific progress.</p>\n<blockquote>\n<p>To put in context how much was $70,000 in 1898,</p>\n<ol>\n<li><p><strong>Real Estate</strong>: Could buy multiple city houses or a large luxury estate.</p>\n</li>\n<li><p><strong>Business Investment</strong>: Sufficient to start a major business or fund several startups.</p>\n</li>\n<li><p><strong>Luxury Goods</strong>: Afforded a lifestyle with high-end clothing, jewelry, and travel.</p>\n</li>\n<li><p><strong>Industrial Projects</strong>: Funded substantial industrial endeavors, like factories or railroads.</p>\n</li>\n<li><p><strong>Salary Comparison</strong>: Equivalent to over a century's earnings for an average worker.</p>\n</li>\n<li><p><strong>Inflation Adjustment</strong>: Comparable to several million dollars in today's currency.</p>\n</li>\n</ol>\n</blockquote>\n<p>Langley's confidence in his project was evident. He was quoted saying, \"The flying machine which will really fly might be evolved by the combined and continuous efforts of mathematicians and mechanicians in from one million to ten million years.\" Despite his skepticism about the practicality of flight, he believed strongly in his own project's success. The public and the press were captivated by Langley's stature and his project's grandeur, overshadowing the efforts of the Wright brothers.</p>\n<p>The Wright brothers' journey was marked by trial and error, a testament to their resilience and ingenuity. While Langley relied on theory, the Wrights focused on practice. They built a wind tunnel in their workshop, through which they tested over 200 wing designs. This hands-on approach allowed them to understand the nuances of flight control and aerodynamics.</p>\n<blockquote>\n<p>If we worked on the assumption that what is accepted as true really is true, then there would be little hope for advance.</p>\n</blockquote>\n<p>Orville Wright once remarked, \"If we worked on the assumption that what is accepted as true really is true, then there would be little hope for advance.\" This philosophy drove them to challenge existing theories and to invent the three-axis control system, crucial for the maneuvering of an aircraft. This system became the standard for all future airplanes, a testament to their innovative spirit.</p>\n<p>As the Wright brothers continued their experiments, Langley's project faced setbacks. The Aerodrome failed twice during public tests, falling into the Potomac River. These failures, combined with the Wright brothers' quiet but steady progress, began to shift the narrative.</p>\n<p>Langley, aware of the Wright brothers' efforts, dismissed them due to their lack of formal training and scientific background. He maintained a facade of superiority, while internally, his confidence was shaken by the brothers' advancements. The press, too, began to question Langley's capabilities, as the Wrights' successes, though not widely publicized, started to surface in aviation circles.</p>\n<p>December 17, 1903, marked a pivotal moment in human history. On this day, at Kitty Hawk, North Carolina, the Wright brothers achieved the first controlled, powered flight. The Flyer, their aircraft, stayed airborne for 12 seconds, covering a distance of 120 feet. This monumental achievement, however, was initially met with skepticism and little fanfare.</p>\n<p>The Wright brothers, undeterred by the lack of immediate recognition, knew they had achieved something extraordinary. Orville Wright later reflected, \"The airplane stays up because it doesn't have the time to fall.\" Their success was a culmination of years of perseverance, a stark contrast to Langley's high-profile yet unsuccessful endeavors.</p>\n<p>Samuel Langley's Aerodrome project, once the darling of the American government and the scientific community, became a symbol of failure. His two public attempts at flight, both ending in crashes, severely damaged his reputation. The press, which once hailed him as a pioneer, now questioned his capabilities.</p>\n<p>Langley, faced with the Wright brothers' success, struggled to regain his standing. His earlier dismissals of the Wrights now seemed baseless. He sought to defend his work, but the tide of public opinion had turned. The contrast between his well-funded project and the Wright brothers' grassroots approach became a narrative of scientific hubris versus practical ingenuity.</p>\n<p>In the years following the Wright brothers' success, the landscape of aviation changed dramatically. The brothers, once unknown, began to gain recognition and acclaim for their groundbreaking work. Their approach to flight, rooted in practical experimentation and a deep understanding of aerodynamics, became the foundation of modern aviation.</p>\n<p>Meanwhile, Langley, in an attempt to salvage his reputation, became more open to collaborations and partnerships. However, the Wright brothers, wary of their past experiences, were hesitant. They had faced skepticism and dismissal from the scientific community, including from Langley, and chose to rely on their own resources and knowledge.</p>\n<p>The story of the Wright brothers and Samuel Pierpont Langley is a powerful narrative about perseverance, innovation, and the triumph of the underdog. It teaches us that true breakthroughs often come from unexpected places. The Wright brothers, with their humble beginnings and lack of formal training, changed the world through their dedication and relentless pursuit of knowledge.</p>\n<p>Their story is a reminder that success is not solely determined by resources or prestige, but by the courage to pursue a vision and the resilience to overcome challenges. It's a testament to the power of determination and the human spirit's capacity for innovation.</p>\n"},"publishedAt":"2024-01-16T06:07:46.475Z","seo":{"title":"160120241139","description":null},"tags":[{"slug":"story"},{"slug":"storytelling"},{"slug":"blog"}]}},{"node":{"id":"6571db8f04eb350fe3bda081","slug":"docker-python-dump-http-request","url":"https://xprilion.com/docker-python-dump-http-request","title":"Docker Python Dump HTTP Request Example","subtitle":null,"brief":"This blog is an explanation for the code present in the github.com/xprilion/docker-python-dump-http-request-example repo.\nThe main purpose of the repo is to create a simple HTTP server developed using Python's Flask framework, hosted in a Docker envi...","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/stock/unsplash/HSACbYjZsqQ/upload/89a45a2d6ddf3a0a53b34fbdf3253d90.jpeg"},"content":{"html":"<p>This blog is an explanation for the code present in the <a target=\"_blank\" href=\"http://github.com/xprilion/docker-python-dump-http-request-example\">github.com/xprilion/docker-python-dump-http-request-example</a> repo.</p>\n<p>The main purpose of the repo is to create a simple HTTP server developed using Python's Flask framework, hosted in a Docker environment. The server is designed to log HTTP request and response data for debugging or informational purposes. The HTTP server receives requests, stores information about them, and then responds with a JSON representation of the request data.</p>\n<h2 id=\"heading-code-explanation\">Code Explanation</h2>\n<p>The repo contains the following files:</p>\n<pre><code class=\"lang-plaintext\">root/\n├── .dockerignore\n├── Dockerfile\n├── main.py\n└── requirements.txt\n</code></pre>\n<p>Let's go over these files in sequence of how they appear in the tree structure.</p>\n<h3 id=\"heading-dockerignorehttpsgithubcomxpriliondocker-python-dump-http-request-exampleblobmaindockerignore\"><a target=\"_blank\" href=\"https://github.com/xprilion/docker-python-dump-http-request-example/blob/main/.dockerignore\">.dockerignore</a></h3>\n<p>The <code>.dockerignore</code> file specifies a pattern for each file or directory that should be ignored when building a Docker image using a Dockerfile. Here's what some lines in our <code>.dockerignore</code> file means:</p>\n<ol>\n<li><p><code>Dockerfile</code>: This line tells Docker to ignore the <code>Dockerfile</code> itself when building the Docker image. Although it might seem strange, this is a common practice. The <code>Dockerfile</code> is used to build the Docker image and is not usually required within the container itself.</p>\n</li>\n<li><p><code>*.pyc, *.pyo, *.pyd</code>: These lines tell Docker to ignore Python compiled files. When Python programs run, they often create compiled versions of the Python source code. These files are not needed to run the Python program, as Python can run the source code directly.</p>\n</li>\n<li><p><code>__pycache__</code>: This line tells Docker to ignore the <strong>pycache</strong> directory. When Python 3.2 and later versions run, they store compiled files in this directory. These files are not needed to run the Python program.</p>\n</li>\n<li><p><code>.pytest_cache</code>: This line tells Docker to ignore the .pytest_cache directory. This directory is created when running tests with pytest, a testing framework for Python. It is not needed to run the Python application, and including it in the Docker image would increase the size of the image without providing any benefits.</p>\n</li>\n</ol>\n<h3 id=\"heading-dockerfilehttpsgithubcomxpriliondocker-python-dump-http-request-exampleblobmaindockerfile\"><a target=\"_blank\" href=\"https://github.com/xprilion/docker-python-dump-http-request-example/blob/main/Dockerfile\">Dockerfile</a></h3>\n<p>This Dockerfile outlines the steps to create a Docker image for a Python application:</p>\n<ol>\n<li><p><code>FROM python:3.11-slim</code>: The base image is the official lightweight Python 3.11 image from Docker Hub.</p>\n</li>\n<li><p><code>ENV PYTHONUNBUFFERED True</code>: Sets the environment variable <code>PYTHONUNBUFFERED</code> to <code>True</code> to allow log messages to be immediately displayed.</p>\n</li>\n<li><p><code>ENV APP_HOME /app</code> and <code>WORKDIR $APP_HOME</code>: Sets <code>/app</code> as the working directory.</p>\n</li>\n<li><p><code>COPY . ./</code>: Copies the local code into the Docker image.</p>\n</li>\n<li><p><code>RUN pip install --no-cache-dir -r requirements.txt</code>: Installs the Python dependencies from <code>requirements.txt</code> file without storing the cache, for a smaller image.</p>\n</li>\n<li><p><code>CMD exec gunicorn --bind :$PORT --workers 1 --threads 8 --timeout 0 main:app</code>: Specifies the command to run when the Docker container is started. The application is served using the Gunicorn HTTP server, binding to the environment variable <code>$PORT</code>, with one worker process and eight threads. The timeout is set to 0 to disable worker timeouts, leaving scaling management to the hosting platform (e.g., Google Cloud Run). The application instance is accessed via <code>main:app</code>, where <code>main</code> is the Python module (file) and <code>app</code> is the Flask application instance.</p>\n</li>\n</ol>\n<h3 id=\"heading-mainpyhttpmainpy\"><a target=\"_blank\" href=\"http://main.py\">main.py</a></h3>\n<p>The <a target=\"_blank\" href=\"http://main.py\"><code>main.py</code></a> script is a Flask application that logs HTTP request and response data. Here's what it does:</p>\n<ol>\n<li><p><strong>Initialization</strong>: A new Flask application is created and debugging is enabled.</p>\n</li>\n<li><p><strong>Request Saving</strong>: The <code>save_request</code> function collects details from the incoming HTTP request, including endpoint, method, cookies, data, headers, arguments, form data, remote address, and file information, if any.</p>\n</li>\n<li><p><strong>Response Saving</strong>: The <code>save_response</code> function collects details from the HTTP response, including status code, status message, headers, and response data.</p>\n</li>\n<li><p><strong>Request and Response Logging</strong>: <code>before_request</code> logs the HTTP method and endpoint of each incoming request. <code>after_request</code> adds CORS headers to the response, logs the response data, and then returns the response.</p>\n</li>\n<li><p><strong>Root Endpoint Handling</strong>: The <code>hello_world</code> function responds to HTTP GET requests at the root (<code>/</code>) endpoint. It generates a unique identifier for the request, saves the request data, creates a response containing the request data in JSON format, sets a cookie, and returns the response.</p>\n</li>\n<li><p><strong>Server Launch</strong>: The server runs on all network interfaces (<code>0.0.0.0</code>) and listens on the port specified by the \"PORT\" environment variable, or 8080 if not set.</p>\n</li>\n</ol>\n<h3 id=\"heading-requirementstxthttpsgithubcomxpriliondocker-python-dump-http-request-exampleblobmainrequirementstxt\"><a target=\"_blank\" href=\"https://github.com/xprilion/docker-python-dump-http-request-example/blob/main/requirements.txt\">requirements.txt</a></h3>\n<p>A <code>requirements.txt</code> file in a Python project is used to specify the dependencies of the project along with their versions (optional).</p>\n<p>In this <code>requirements.txt</code> file, we load Flask and gunicorn, which are enough for us to build the project. In most real world projects there are far more dependencies and the <code>requirements.txt</code> file for such projects may span several dozen lines.</p>\n<p>When setting up the project, you can use <code>pip install -r requirements.txt</code> to install all of the dependencies. If version numbers are provided, pip will try to install the exact version numbers or else default to the latest available version.</p>\n<h2 id=\"heading-code-usage\">Code Usage</h2>\n<p>Now, a quick note on how you can quickly deploy this code to anywhere you want using docker:</p>\n<ol>\n<li><p><strong>Build the Docker image</strong>: Run the following command in the terminal, in the directory containing your Dockerfile. This command builds a Docker image with the tag <code>my-python-app</code>.</p>\n<pre><code class=\"lang-bash\"> docker build -t my-python-app .\n</code></pre>\n</li>\n<li><p><strong>Run the Docker container</strong>: Run the Docker container from the image you just created. Replace <code>$PORT</code> with the port number you want to use.</p>\n<pre><code class=\"lang-bash\"> docker run -p <span class=\"hljs-variable\">$PORT</span>:8080 -e PORT=8080 my-python-app\n</code></pre>\n<p> This command runs the Docker container, mapping the host's <code>$PORT</code> to the container's <code>8080</code> port, which the Flask application inside the Docker container is listening on.</p>\n</li>\n</ol>\n<p>Remember, you need to have Docker installed on your machine to execute these commands.</p>\n<p>In production scenarios, you would push your Docker image to a Docker registry (like Docker Hub, Google Container Registry, or AWS Elastic Container Registry), then pull and run the Docker image from machines where you want your application to run. Also, orchestration tools like Kubernetes, Docker Swarm, or managed services like Google Cloud Run, AWS ECS/Fargate could be used to handle deployment, scaling, and management of Docker containers in a production setting.</p>\n<p>When you hit the endpoint created by deploying this code, you'll get a response similar to the one below:</p>\n<pre><code class=\"lang-json\">{\n    <span class=\"hljs-attr\">\"uuid\"</span>: <span class=\"hljs-string\">\"f50cc7d0f26511eda4bb75c989e55f1e\"</span>,\n    <span class=\"hljs-attr\">\"endpoint\"</span>: <span class=\"hljs-string\">\"hello_world\"</span>,\n    <span class=\"hljs-attr\">\"method\"</span>: <span class=\"hljs-string\">\"GET\"</span>,\n    <span class=\"hljs-attr\">\"cookies\"</span>: {\n        <span class=\"hljs-attr\">\"cookie-name\"</span>: <span class=\"hljs-string\">\"cookie-value\"</span>\n    },\n    <span class=\"hljs-attr\">\"data\"</span>: <span class=\"hljs-string\">\"b''\"</span>,\n    <span class=\"hljs-attr\">\"headers\"</span>: {\n        <span class=\"hljs-attr\">\"Host\"</span>: <span class=\"hljs-string\">\"test-cloud-run-custom.xpri.dev\"</span>,\n        <span class=\"hljs-attr\">\"Cache-Control\"</span>: <span class=\"hljs-string\">\"max-age=0\"</span>,\n        <span class=\"hljs-attr\">\"Sec-Ch-Ua\"</span>: <span class=\"hljs-string\">\"\\\"Chromium\\\";v=\\\"112\\\", \\\"Google Chrome\\\";v=\\\"112\\\", \\\"Not:A-Brand\\\";v=\\\"99\\\"\"</span>,\n        <span class=\"hljs-attr\">\"Sec-Ch-Ua-Mobile\"</span>: <span class=\"hljs-string\">\"?0\"</span>,\n        <span class=\"hljs-attr\">\"Sec-Ch-Ua-Platform\"</span>: <span class=\"hljs-string\">\"\\\"macOS\\\"\"</span>,\n        <span class=\"hljs-attr\">\"Upgrade-Insecure-Requests\"</span>: <span class=\"hljs-string\">\"1\"</span>,\n        <span class=\"hljs-attr\">\"User-Agent\"</span>: <span class=\"hljs-string\">\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\"</span>,\n        <span class=\"hljs-attr\">\"Accept\"</span>: <span class=\"hljs-string\">\"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\"</span>,\n        <span class=\"hljs-attr\">\"Sec-Ch-Ua-Arch\"</span>: <span class=\"hljs-string\">\"\\\"arm\\\"\"</span>,\n        <span class=\"hljs-attr\">\"Sec-Ch-Ua-Platform-Version\"</span>: <span class=\"hljs-string\">\"\\\"13.3.1\\\"\"</span>,\n        <span class=\"hljs-attr\">\"Sec-Ch-Ua-Model\"</span>: <span class=\"hljs-string\">\"\\\"\\\"\"</span>,\n        <span class=\"hljs-attr\">\"Sec-Ch-Ua-Bitness\"</span>: <span class=\"hljs-string\">\"\\\"64\\\"\"</span>,\n        <span class=\"hljs-attr\">\"Sec-Ch-Ua-Wow64\"</span>: <span class=\"hljs-string\">\"?0\"</span>,\n        <span class=\"hljs-attr\">\"Sec-Ch-Ua-Full-Version-List\"</span>: <span class=\"hljs-string\">\"\\\"Chromium\\\";v=\\\"112.0.5615.137\\\", \\\"Google Chrome\\\";v=\\\"112.0.5615.137\\\", \\\"Not:A-Brand\\\";v=\\\"99.0.0.0\\\"\"</span>,\n        <span class=\"hljs-attr\">\"Sec-Fetch-Site\"</span>: <span class=\"hljs-string\">\"none\"</span>,\n        <span class=\"hljs-attr\">\"Sec-Fetch-Mode\"</span>: <span class=\"hljs-string\">\"navigate\"</span>,\n        <span class=\"hljs-attr\">\"Sec-Fetch-User\"</span>: <span class=\"hljs-string\">\"?1\"</span>,\n        <span class=\"hljs-attr\">\"Sec-Fetch-Dest\"</span>: <span class=\"hljs-string\">\"document\"</span>,\n        <span class=\"hljs-attr\">\"Accept-Language\"</span>: <span class=\"hljs-string\">\"en-US,en;q=0.9\"</span>,\n        <span class=\"hljs-attr\">\"X-Cloud-Trace-Context\"</span>: <span class=\"hljs-string\">\"7361fa590088de543df325a9832ab85e/4131215564459013469\"</span>,\n        <span class=\"hljs-attr\">\"Traceparent\"</span>: <span class=\"hljs-string\">\"00-7361fa590088de543df325a9832ab85e-395506aaf224f55d-00\"</span>,\n        <span class=\"hljs-attr\">\"X-Forwarded-For\"</span>: <span class=\"hljs-string\">\"2401:4900:xxxx:xxxx:xxxx:5400:xxxx:xxxx\"</span>,\n        <span class=\"hljs-attr\">\"X-Forwarded-Proto\"</span>: <span class=\"hljs-string\">\"https\"</span>,\n        <span class=\"hljs-attr\">\"Forwarded\"</span>: <span class=\"hljs-string\">\"for=\\\"[2401:4900:xxxx:xxxx:xxxx:5400:xxxx:xxxx]\\\";proto=https\"</span>,\n        <span class=\"hljs-attr\">\"Accept-Encoding\"</span>: <span class=\"hljs-string\">\"gzip, deflate, br\"</span>\n    },\n    <span class=\"hljs-attr\">\"args\"</span>: {},\n    <span class=\"hljs-attr\">\"form\"</span>: {},\n    <span class=\"hljs-attr\">\"remote_addr\"</span>: <span class=\"hljs-string\">\"169.xxx.xxx.xxx\"</span>,\n    <span class=\"hljs-attr\">\"files\"</span>: []\n}\n</code></pre>\n<p>This response is a JSON representation of the details of an HTTP GET request received by the server. Let's break down what each key-value pair means:</p>\n<ul>\n<li><p><code>\"uuid\": \"f50cc7d0f26511eda4bb75c989e55f1e\"</code>: This is a unique identifier for this request.</p>\n</li>\n<li><p><code>\"endpoint\": \"hello_world\"</code>: The name of the endpoint that was hit. It corresponds to the function handling the request in the Flask application.</p>\n</li>\n<li><p><code>\"method\": \"GET\"</code>: The HTTP method used for this request.</p>\n</li>\n<li><p><code>\"cookies\": {\"cookie-name\": \"cookie-value\"}</code>: The cookies sent with this request. In this case, there is one cookie named \"cookie-name\" with a value of \"cookie-value\".</p>\n</li>\n<li><p><code>\"data\": \"b''\"</code>: The data sent with this request. In this case, no data was sent.</p>\n</li>\n<li><p><code>\"headers\"</code>: This is a dictionary containing all the HTTP headers that were sent with the request. This includes information about the client, the accepted response types, and more.</p>\n</li>\n<li><p><code>\"args\": {}</code> and <code>\"form\": {}</code>: These would contain any query parameters or form data sent with the request. In this case, none were sent.</p>\n</li>\n<li><p><code>\"remote_addr\": \"169.254.1.1\"</code>: This is the IP address of the client that sent the request.</p>\n</li>\n<li><p><code>\"files\": []</code>: This would contain any files sent with the request. In this case, none were sent.</p>\n</li>\n</ul>\n<p>The server logs all of these details for each request it receives. This can be useful for debugging, analytics, and other purposes.</p>\n"},"publishedAt":"2023-05-13T06:30:00.000Z","seo":{"title":"130520231708","description":null},"tags":[{"slug":"blog"}]}},{"node":{"id":"6437e7b7d53a5d2afe788b6b","slug":"job-scheduling-on-google-cloud-platform","url":"https://xprilion.com/job-scheduling-on-google-cloud-platform","title":"Job Scheduling on Google Cloud Platform","subtitle":null,"brief":"Job scheduling is like conducting an orchestra; every task must play its part at the right time to create a harmonious symphony of efficiency.\n\nIn the world of software systems, job scheduling plays a critical role in ensuring efficient task executio...","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1681385149642/afd6bd17-834e-4204-9edf-d2c8e0c3ffa4.png"},"content":{"html":"<blockquote>\n<p>Job scheduling is like conducting an orchestra; every task must play its part at the right time to create a harmonious symphony of efficiency.</p>\n</blockquote>\n<p>In the world of software systems, job scheduling plays a critical role in ensuring efficient task execution and harmonious resource management. In this article, we'll explore what job scheduling is, the different types of scheduled jobs and how to implement job scheduling using different available methods. We'll mostly be focusing on the tools and services available for Job Scheduling on Google Cloud Platform. However, these concepts can be extended to any platform with slight modifications.</p>\n<h2 id=\"heading-scheduling-with-system-crons-a-classic-approach\">Scheduling with System Crons: A Classic Approach</h2>\n<p>For those who prefer a more traditional approach, system crons can be used for job scheduling. This method involves creating a cron job on a Compute Engine instance or Kubernetes cluster. Although system crons lack some of the features provided by GCP's managed services, they can still be a useful tool for basic scheduling tasks.</p>\n<h3 id=\"heading-example-database-maintenance-with-system-cron\">Example: Database Maintenance with System Cron</h3>\n<p>In this example, we'll create a simple cron job on a Compute Engine instance to perform periodic database maintenance.</p>\n<p>First, SSH into your Compute Engine instance:</p>\n<pre><code class=\"lang-bash\">gcloud compute ssh my-instance --zone my-instance-zone\n</code></pre>\n<p>Next, open the crontab editor to create a new cron job:</p>\n<pre><code class=\"lang-bash\">crontab -e\n</code></pre>\n<p>Add the following line to the crontab file to schedule a database maintenance script to run every Sunday at 3 AM:</p>\n<pre><code class=\"lang-bash\">0 3 * * 7 /path/to/your/db-maintenance-script.sh\n</code></pre>\n<p>Save and exit the editor. The cron job is now scheduled to run the <a target=\"_blank\" href=\"http://db-maintenance-script.sh\"><code>db-maintenance-script.sh</code></a> script every Sunday at 3 AM.</p>\n<p>Remember that system crons depend on the availability and reliability of your Compute Engine instance. If the instance experiences downtime or other issues, your scheduled tasks may not execute as expected. When using system crons, it's essential to implement monitoring and error handling to maintain the reliability of your scheduling solution.</p>\n<p>Keep in mind that using system crons may require more manual management compared to GCP's managed services. However, they can still be a viable option for simple scheduling tasks that don't require the advanced features provided by GCP's job scheduling components.</p>\n<h3 id=\"heading-understanding-cron-job-timing-parameters\">Understanding Cron Job Timing Parameters</h3>\n<p>Cron job scheduling is based on a series of timing parameters that determine when a task should be executed. A cron job's timing is represented by a string containing five fields separated by spaces: minute, hour, day of the month, month, and day of the week.</p>\n<p>Each field can have a specific value, a range of values, or a wildcard (*) to represent all possible values. Here's a brief explanation of each field:</p>\n<ul>\n<li><p><strong>Minute (0-59)</strong>: Specifies the minute when the task should be executed.</p>\n</li>\n<li><p><strong>Hour (0-23)</strong>: Specifies the hour when the task should be executed.</p>\n</li>\n<li><p><strong>Day of the month (1-31)</strong>: Specifies the day of the month when the task should be executed.</p>\n</li>\n<li><p><strong>Month (1-12 or JAN-DEC)</strong>: Specifies the month when the task should be executed. You can use either numeric values or three-letter month abbreviations.</p>\n</li>\n<li><p><strong>Day of the week (0-7 or SUN-SAT)</strong>: Specifies the day of the week when the task should be executed. Both 0 and 7 represent Sunday. You can use either numeric values or three-letter day abbreviations.</p>\n</li>\n</ul>\n<p>Here are some examples of cron job timing parameters and their meanings:</p>\n<p><code>0 3 * * *</code>: This schedule runs the task every day at 3 AM.</p>\n<p><code>0 3 * * 7</code>: This schedule runs the task every Sunday at 3 AM.</p>\n<p><code>0 3 1 * *</code>: This schedule runs the task at 3 AM on the first day of every month.</p>\n<p><code>*/15 * * * *</code>: This schedule runs the task every 15 minutes.</p>\n<p>Understanding the cron job timing parameters allows you to create precise schedules for your tasks, ensuring that they run exactly when needed.</p>\n<h2 id=\"heading-gcp-job-scheduling-components\">GCP Job Scheduling Components</h2>\n<p>GCP provides several key components for job scheduling:</p>\n<ul>\n<li><p><a target=\"_blank\" href=\"https://cloud.google.com/scheduler\">Cloud Scheduler</a>: A managed cron service for time-based tasks</p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://cloud.google.com/functions\">Cloud Functions</a>: A serverless platform for event-driven functions</p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://cloud.google.com/pubsub\">Cloud Pub/Sub</a>: A global messaging service for event-driven architectures</p>\n</li>\n<li><p><a target=\"_blank\" href=\"https://cloud.google.com/workflows\">Cloud Workflows</a>: A serverless workflow orchestration service</p>\n</li>\n</ul>\n<p>These components are crucial for creating efficient scheduling solutions. For an in-depth understanding of each component, consult the <a target=\"_blank\" href=\"https://cloud.google.com/products\">GCP documentation</a>.</p>\n<h3 id=\"heading-scheduling-strategies-time-based-event-based-and-hybrid\">Scheduling Strategies: Time-based, Event-based, and Hybrid</h3>\n<p>GCP offers three primary scheduling strategies, allowing you to select the best fit for your needs:</p>\n<ol>\n<li><p><strong>Time-based scheduling</strong> with Cloud Scheduler: Ideal for tasks that need to run at regular intervals or specific times, such as backups or maintenance jobs.</p>\n</li>\n<li><p><strong>Event-based scheduling</strong> with Cloud Functions and Cloud Pub/Sub: Perfect for tasks that need to be executed in response to specific events, such as user actions or data updates.</p>\n</li>\n<li><p><strong>Hybrid scheduling</strong>: A flexible and dynamic job scheduling system that adapts to various scenarios and needs by combining both time-based and event-based approaches.</p>\n</li>\n</ol>\n<h3 id=\"heading-best-practices-for-job-scheduling-in-gcp\">Best Practices for Job Scheduling in GCP</h3>\n<p>To ensure optimal performance and reliability, consider the following best practices:</p>\n<ul>\n<li><p><strong>Monitoring and Logging</strong>: Utilize tools like <a target=\"_blank\" href=\"https://cloud.google.com/monitoring\">Cloud Monitoring</a> and <a target=\"_blank\" href=\"https://cloud.google.com/logging\">Cloud Logging</a> to keep a close eye on the performance and progress of your scheduled tasks.</p>\n</li>\n<li><p><strong>Error Handling and Retries</strong>: Implement error handling and retries in your job scheduling process to maintain system reliability and minimize the impact of failures.</p>\n</li>\n<li><p><strong>Resource Allocation</strong>: Leverage GCP tools for load balancing and autoscaling to optimize resource usage and maintain system performance.</p>\n</li>\n<li><p><strong>Security Considerations</strong>: Follow best practices and use GCP's security features to protect your scheduled tasks and ensure the overall security of your system.</p>\n</li>\n</ul>\n<h2 id=\"heading-enhancing-job-scheduling-with-google-cloud-operations-suite\">Enhancing Job Scheduling with Google Cloud Operations Suite</h2>\n<p>Google Cloud Operations Suite, formerly known as Stackdriver, is a comprehensive suite of tools that helps developers monitor, troubleshoot, and optimize their applications on GCP. While not a job scheduling tool itself, Cloud Operations Suite can significantly enhance job scheduling by providing insights into application performance, resource usage, and potential issues.</p>\n<ul>\n<li><p><strong>Monitoring</strong>: With Cloud Monitoring, you can set up custom dashboards to track metrics, logs, and traces from your job scheduling components such as Cloud Functions, Cloud Scheduler, and Cloud Pub/Sub.</p>\n</li>\n<li><p><strong>Logging</strong>: Cloud Logging enables you to store, search, and analyze log data from your scheduled tasks, making it easier to identify and resolve issues.</p>\n</li>\n<li><p><strong>Error Reporting</strong>: Cloud Error Reporting automatically detects and reports errors from your applications, allowing you to quickly identify and resolve issues affecting your scheduled tasks.</p>\n</li>\n<li><p><strong>Trace</strong>: Cloud Trace helps you analyze the performance of your applications by collecting and visualizing latency data from your job scheduling components.</p>\n</li>\n</ul>\n<p>By integrating Google Cloud Operations Suite with your job scheduling components, you can gain valuable insights into your application's performance and optimize your scheduling strategy for better efficiency and reliability.</p>\n<h2 id=\"heading-streamline-event-driven-job-scheduling-with-eventarc\">Streamline Event-driven Job Scheduling with Eventarc</h2>\n<p>Eventarc is a GCP service that simplifies event-driven job scheduling by allowing you to route events from various sources to Google Cloud services or custom targets. It enables you to create event-driven applications that can react to changes in your environment, such as new files being added to Cloud Storage or updates in a Firestore database.</p>\n<p>With Eventarc, you can create event-based job scheduling solutions that trigger Cloud Functions, Cloud Run services, or custom webhooks based on specific events. For example, you could set up a Cloud Function to process new images uploaded to Cloud Storage or trigger a Cloud Run service to handle updates in a Firestore database.</p>\n<p>Here's how you can create a trigger using Eventarc:</p>\n<ol>\n<li>Enable the Eventarc API:</li>\n</ol>\n<pre><code class=\"lang-bash\">gcloud services <span class=\"hljs-built_in\">enable</span> eventarc.googleapis.com\n</code></pre>\n<ol>\n<li>Create an Eventarc trigger:</li>\n</ol>\n<pre><code class=\"lang-bash\">gcloud eventarc triggers create my-trigger --destination-run-service=my-cloud-run-service --destination-run-region=my-region --event-filters=<span class=\"hljs-string\">\"type=google.cloud.pubsub.topic.v1.messagePublished\"</span>\n</code></pre>\n<p>This command creates an Eventarc trigger named <code>my-trigger</code> that listens for messages published to a Cloud Pub/Sub topic and sends the event to the specified Cloud Run service in the given region.</p>\n<p>By leveraging Eventarc in your job scheduling strategy, you can create more dynamic, event-driven applications that automatically react to changes in your environment, improving efficiency and reducing the need for manual intervention.</p>\n<h2 id=\"heading-practical-examples-of-job-scheduling-in-gcp\">Practical Examples of Job Scheduling in GCP</h2>\n<p>To better understand the practical applications of GCP's scheduling strategies, let's dive into some real-world examples:</p>\n<h3 id=\"heading-time-based-example-automated-backups\">Time-based Example: Automated Backups</h3>\n<p>Automate backups of your system using Cloud Scheduler to ensure data protection and recovery. To create a Cloud Scheduler job that triggers a backup, use the following <code>gcloud</code> command:</p>\n<pre><code class=\"lang-bash\">gcloud scheduler <span class=\"hljs-built_in\">jobs</span> create http my-backup-job --schedule <span class=\"hljs-string\">\"0 1 * * *\"</span> --http-method POST --uri https://example.com/backup --oidc-service-account-email my-sa@example.iam.gserviceaccount.com\n</code></pre>\n<p>In this example, <code>my-backup-job</code> is a backup job that runs daily at 1 AM, making a POST request to <a target=\"_blank\" href=\"https://example.com/backup\"><code>https://example.com/backup</code></a>.</p>\n<h3 id=\"heading-event-based-example-real-time-data-processing\">Event-based Example: Real-time Data Processing</h3>\n<p>Implement real-time data processing using Cloud Functions and Cloud Pub/Sub. For instance, you can create a Cloud Function that processes incoming data and a Cloud Pub/Sub topic to trigger the function.</p>\n<p>First, deploy the Cloud Function:</p>\n<pre><code class=\"lang-bash\">gcloud <span class=\"hljs-built_in\">functions</span> deploy processData --runtime nodejs14 --trigger-topic my-data-topic --entry-point processDataFunction\n</code></pre>\n<p>In this example, <code>processData</code> is the function that processes incoming data, triggered by the <code>my-data-topic</code> Cloud Pub/Sub topic.</p>\n<p>Then, publish a message to the Cloud Pub/Sub topic:</p>\n<pre><code class=\"lang-bash\">gcloud pubsub topics publish my-data-topic --message <span class=\"hljs-string\">'{\"data\": \"your-data-here\"}'</span>\n</code></pre>\n<p>This command publishes a message containing data to the <code>my-data-topic</code> topic, which triggers the <code>processData</code> function.</p>\n<p>The code for this function has not been included in this blog for sake of focus on the topic. You can check it out in this <a target=\"_blank\" href=\"https://gist.github.com/xprilion/b2b763e11c3d4a51a90e2dfa8f49210d\">Github Gist</a>.</p>\n<h3 id=\"heading-hybrid-example-inventory-management\">Hybrid Example: Inventory Management</h3>\n<p>Manage inventory with hybrid scheduling, using time-based tasks for restocking and event-based tasks for real-time updates. For instance, you can create a Cloud Scheduler job for periodic restocking and a Cloud Function triggered by Cloud Pub/Sub for processing real-time inventory updates.</p>\n<p>First, create the Cloud Scheduler job for restocking:</p>\n<pre><code class=\"lang-bash\">gcloud scheduler <span class=\"hljs-built_in\">jobs</span> create http restock-job --schedule <span class=\"hljs-string\">\"0 2 * * 1\"</span> --http-method POST --uri https://example.com/restock --oidc-service-account-email my-sa@example.iam.gserviceaccount.com\n</code></pre>\n<p>In this example, <code>restock-job</code> is a restocking job that runs every Monday at 2 AM, making a POST request to <a target=\"_blank\" href=\"https://example.com/restock\"><code>https://example.com/restock</code></a>.</p>\n<p>Next, deploy the Cloud Function for real-time inventory updates:</p>\n<pre><code class=\"lang-bash\">gcloud <span class=\"hljs-built_in\">functions</span> deploy updateInventory --runtime nodejs14 --trigger-topic inventory-updates --entry-point updateInventoryFunction\n</code></pre>\n<p>In this example, <code>updateInventory</code> is the function that processes real-time inventory updates, triggered by the <code>inventory-updates</code> Cloud Pub/Sub topic.</p>\n<p>Finally, publish a message to the Cloud Pub/Sub topic for inventory updates:</p>\n<pre><code class=\"lang-bash\">gcloud pubsub topics publish inventory-updates --message <span class=\"hljs-string\">'{\"update\": \"your-update-here\"}'</span>\n</code></pre>\n<p>This command publishes a message containing an inventory update to the <code>inventory-updates</code> topic, which triggers the <code>updateInventory</code> function.</p>\n<h2 id=\"heading-conclusion\">Conclusion</h2>\n<p>GCP offers a robust set of tools for efficient job scheduling, ensuring optimal resource usage, scalability, and fault tolerance. By selecting the appropriate scheduling strategy and following best practices, developers can craft robust and efficient scheduling solutions tailored to their specific needs. With a deeper understanding of GCP job scheduling, you can unlock the full potential of your applications and tackle even the most complex systems.</p>\n<p>Happy scheduling!</p>\n"},"publishedAt":"2023-04-13T11:29:59.740Z","seo":{"title":"130420231708","description":null},"tags":[{"slug":"google-cloud"},{"slug":"cronjob"},{"slug":"gcp"},{"slug":"software-development"},{"slug":"blog"}]}},{"node":{"id":"6571975384a627f7f38fc6e8","slug":"python-websockets-ssl-with-lets-encrypt","url":"https://xprilion.com/python-websockets-ssl-with-lets-encrypt","title":"Python Websockets SSL with Let's Encrypt","subtitle":null,"brief":"This tutorial is an explanation of my gist Python Websockets SSL with Let's Encrypt .\nWith the launch of HTML5 in 2008, a technology that immediately took off in popularity was WebSockets. According to W3C, the basic definition of a Websocket is - an...","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1701944277424/c11b5ea0-e49e-4596-a790-0a9ea2675142.png"},"content":{"html":"<p>This tutorial is an explanation of my gist <a target=\"_blank\" href=\"https://gist.github.com/xprilion/ceab48ec77a70be1d403e396170991e6\">Python Websockets SSL with Let's Encrypt</a> .</p>\n<p>With the launch of HTML5 in 2008, a technology that immediately took off in popularity was WebSockets. According to W3C, the basic definition of a Websocket is - an interface that enables web applications to maintain bidirectional communications with server-side processes.</p>\n<p>In this short tutorial, I'll be showing you how you can host a WebSocket server with SSL enabled on it. This allows your socket server to run on an HTTPS address.</p>\n<h2 id=\"heading-setup\">Setup</h2>\n<p>We'll be working with <code>asyncio</code> library and for WebSocket server implementation will be using the <code>websockets</code> library.</p>\n<p>The <code>asyncio</code> library comes pre-packaged with Python distributions since Python 3.4. To install <code>websockets</code> library, you can use the following command:</p>\n<pre><code class=\"lang-bash\">pip install websockets\n</code></pre>\n<p>Next, we'll be looking to how to generate the SSL certificate files.</p>\n<h2 id=\"heading-generate-certificate-and-keyfile-using-lets-encrypt\">Generate certificate and keyfile using Let's Encrypt</h2>\n<p>Before you can enable SSL on your WebSocket being run by a Python script, you'll have to generate certificate files for your domain.</p>\n<p>The basic gist of this step is to fetch Let's Encrypt signed certficiates for your domain and store them on your server where the Python WebSocket script is running.</p>\n<p>Here's a great quick tutorial on <a target=\"_blank\" href=\"https://www.digitalocean.com/community/tutorials/how-to-use-certbot-standalone-mode-to-retrieve-let-s-encrypt-ssl-certificates-on-ubuntu-20-04\">How To Use Certbot Standalone Mode to Retrieve Let's Encrypt SSL Certificates on Ubuntu 20.04</a>.</p>\n<h2 id=\"heading-make-certificate-files-accessible\">Make certificate files accessible</h2>\n<p>After generating the files correctly, you need to make them accessible to the current user who runs the Python WebSocket script.</p>\n<p>In the previous step, if <code>certbot</code> stored your certificate files at <code>/etc/letsencrypt/live/your_domain</code> location, you should be able to see 4 files when you perform an <code>ls</code> on the folder -</p>\n<pre><code class=\"lang-bash\">~$ ls /etc/letsencrypt/live/your_domain\ncert.pem  chain.pem  fullchain.pem  privkey.pem  README\n</code></pre>\n<p>To change the owner of the certificate files, use the following command:</p>\n<pre><code class=\"lang-bash\">~$ sudo chown -R $(id -u):$(id -g) /etc/letsencrypt/live/your_domain\n</code></pre>\n<p>Next, ensure that the right permissions are applied to the folder:</p>\n<pre><code class=\"lang-bash\">~$ sudo chmod -R 400 /etc/letsencrypt/live/your_domain\n</code></pre>\n<p>We're now good to read these files from the Python WebSocket script.</p>\n<h2 id=\"heading-create-server-script\">Create server script</h2>\n<p>While your WebSocket server script will differ from the most barebones implementation, here's one for you -</p>\n<p>Create a file named <code>socket_</code><a target=\"_blank\" href=\"http://server.py\"><code>server.py</code></a>. Then, make all the necessary imports.</p>\n<pre><code class=\"lang-python\"><span class=\"hljs-comment\">#!/usr/bin/env python</span>\n\n<span class=\"hljs-comment\"># WS server example that synchronizes state across clients</span>\n\n<span class=\"hljs-keyword\">import</span> asyncio\n<span class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">import</span> logging\n<span class=\"hljs-keyword\">import</span> websockets\n<span class=\"hljs-keyword\">import</span> ssl\n\nlogging.basicConfig()\n</code></pre>\n<p>Next, let us configure the SSL for this script, as shown below -</p>\n<pre><code class=\"lang-python\">ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n\n<span class=\"hljs-comment\"># Generate with Lets Encrypt, chown to current user and 400 permissions</span>\nssl_cert = <span class=\"hljs-string\">\"/etc/letsencrypt/live/your_domain/fullchain.pem\"</span>\nssl_key = <span class=\"hljs-string\">\"/etc/letsencrypt/live/your_domain/privkey.pem\"</span>\n\nssl_context.load_cert_chain(ssl_cert, keyfile=ssl_key)\n</code></pre>\n<p>The above code configures the script to run a TLS server with the certificates available in the folder generated by <code>certbot</code>.</p>\n<p>Next, we define the functions that shall be used to notify clients of the socket server and to get information from them.</p>\n<pre><code class=\"lang-python\">STATE = {<span class=\"hljs-string\">\"value\"</span>: <span class=\"hljs-number\">0</span>}\n\nUSERS = set()\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">state_event</span>():</span>\n    <span class=\"hljs-keyword\">return</span> json.dumps({<span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"state\"</span>, **STATE})\n\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">users_event</span>():</span>\n    <span class=\"hljs-keyword\">return</span> json.dumps({<span class=\"hljs-string\">\"type\"</span>: <span class=\"hljs-string\">\"users\"</span>, <span class=\"hljs-string\">\"count\"</span>: len(USERS)})\n\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">notify_state</span>():</span>\n    <span class=\"hljs-keyword\">if</span> USERS:  <span class=\"hljs-comment\"># asyncio.wait doesn't accept an empty list</span>\n        message = state_event()\n        <span class=\"hljs-keyword\">await</span> asyncio.wait([user.send(message) <span class=\"hljs-keyword\">for</span> user <span class=\"hljs-keyword\">in</span> USERS])\n\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">notify_users</span>():</span>\n    <span class=\"hljs-keyword\">if</span> USERS:  <span class=\"hljs-comment\"># asyncio.wait doesn't accept an empty list</span>\n        message = users_event()\n        <span class=\"hljs-keyword\">await</span> asyncio.wait([user.send(message) <span class=\"hljs-keyword\">for</span> user <span class=\"hljs-keyword\">in</span> USERS])\n</code></pre>\n<p>After that, we need to add functions that register and unregister clients from the WebSocket.</p>\n<pre><code class=\"lang-python\"><span class=\"hljs-keyword\">async</span> <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">register</span>(<span class=\"hljs-params\">websocket</span>):</span>\n    USERS.add(websocket)\n    <span class=\"hljs-keyword\">await</span> notify_users()\n\n\n<span class=\"hljs-keyword\">async</span> <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">unregister</span>(<span class=\"hljs-params\">websocket</span>):</span>\n    USERS.remove(websocket)\n    <span class=\"hljs-keyword\">await</span> notify_users()\n</code></pre>\n<p>This done, let us implement a function that gets state update requests from clients and performs it.</p>\n<pre><code class=\"lang-python\"><span class=\"hljs-keyword\">async</span> <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">counter</span>(<span class=\"hljs-params\">websocket, path</span>):</span>\n    <span class=\"hljs-comment\"># register(websocket) sends user_event() to websocket</span>\n    <span class=\"hljs-keyword\">await</span> register(websocket)\n    <span class=\"hljs-keyword\">try</span>:\n        <span class=\"hljs-keyword\">await</span> websocket.send(state_event())\n        <span class=\"hljs-keyword\">async</span> <span class=\"hljs-keyword\">for</span> message <span class=\"hljs-keyword\">in</span> websocket:\n            data = json.loads(message)\n            <span class=\"hljs-keyword\">if</span> data[<span class=\"hljs-string\">\"action\"</span>] == <span class=\"hljs-string\">\"minus\"</span>:\n                STATE[<span class=\"hljs-string\">\"value\"</span>] -= <span class=\"hljs-number\">1</span>\n                <span class=\"hljs-keyword\">await</span> notify_state()\n            <span class=\"hljs-keyword\">elif</span> data[<span class=\"hljs-string\">\"action\"</span>] == <span class=\"hljs-string\">\"plus\"</span>:\n                STATE[<span class=\"hljs-string\">\"value\"</span>] += <span class=\"hljs-number\">1</span>\n                <span class=\"hljs-keyword\">await</span> notify_state()\n            <span class=\"hljs-keyword\">else</span>:\n                logging.error(<span class=\"hljs-string\">\"unsupported event: {}\"</span>, data)\n    <span class=\"hljs-keyword\">finally</span>:\n        <span class=\"hljs-keyword\">await</span> unregister(websocket)\n</code></pre>\n<p>Finally, we run the server.</p>\n<pre><code class=\"lang-python\">start_server = websockets.serve(counter, <span class=\"hljs-string\">\"0.0.0.0\"</span>, <span class=\"hljs-number\">6789</span>, ssl=ssl_context)\n\nasyncio.get_event_loop().run_until_complete(start_server)\nasyncio.get_event_loop().run_forever()\n</code></pre>\n<p>How you expose the server to the internet, I shall leave that on your use case. However, you can explore this tutorial on <a target=\"_blank\" href=\"https://websockets.readthedocs.io/en/stable/howto/nginx.html\">Websockets - Deploy behind nginx</a>.</p>\n<h2 id=\"heading-implement-a-client-page\">Implement a client page</h2>\n<p>To test the above WebSocket script, you can spin up your own client script or feel free to use the <a target=\"_blank\" href=\"https://gist.github.com/xprilion/ceab48ec77a70be1d403e396170991e6#file-socket_client-html\">socket_client.html</a> file I've provided.</p>\n<p>Make sure to update the following line in the client file to point to your live server -</p>\n<pre><code class=\"lang-javascript\">websocket = <span class=\"hljs-keyword\">new</span> WebSocket(<span class=\"hljs-string\">\"wss://localhost:6789/\"</span>);\n</code></pre>\n<p>Note that we're using the <code>wss://</code> protocol here instead of <code>ws://</code> protocol which is popularly found on other tutorials on the internet.</p>\n<h2 id=\"heading-conclusion\">Conclusion</h2>\n<p>This was my attempt at explaining the gist I put out more than an year back. Hope it helps you go through the process easier than how I had first written it.</p>\n<p>Make sure to leave me feedback on how this blog went!</p>\n"},"publishedAt":"2022-11-20T18:30:00.000Z","seo":{"title":"211120221708","description":null},"tags":[{"slug":"blog"}]}},{"node":{"id":"6571da74cabbbca2f1bf829e","slug":"high-availability-ml-deployments","url":"https://xprilion.com/high-availability-ml-deployments","title":"High Availability ML Deployments","subtitle":null,"brief":"The average cost of IT downtime is $5,600 per minute.\n~ Gartner\n\nDowntimes can be costly. During downtimes, a company may face loss of business, loss of customer trust, loss of reputation in the technical and business community, or even all of these ...","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1701960031697/d3227ce0-d061-43db-a517-a27f364666b8.png"},"content":{"html":"<blockquote>\n<p>The average cost of IT downtime is $5,600 per minute.</p>\n<p>~ Gartner</p>\n</blockquote>\n<p>Downtimes can be costly. During downtimes, a company may face loss of business, loss of customer trust, loss of reputation in the technical and business community, or even all of these together. Downtimes are not fun, and until it happens to us, we all tend to think it cannot happen to us.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701960112859/5ef3fcf5-6c79-4b1b-b2d5-a62dab91504c.png\" alt class=\"image--center mx-auto\" /></p>\n<p>What makes the challenge more difficult is the fact that the world is rapidly incorporating more and more Machine learning on the web, and this has led to complexities that did not exist with non-ML software solutions.</p>\n<p>In this blog, we'll be exploring how to design a machine learning based solution that is highly reliable and does not suffer much when downtimes occur.</p>\n<p>In this 2 part article, we’ll try to find answers to the following questions:</p>\n<ul>\n<li><p>How quickly can your system bounce back from disasters?</p>\n</li>\n<li><p>Are your ML deployments resilient?</p>\n</li>\n<li><p>When can you call your system architecture “high availability”?</p>\n</li>\n</ul>\n<h2 id=\"heading-part-1-undestanding-hadr-systems\">Part 1 - Undestanding HADR systems</h2>\n<p>Let us begin by understanding a few basics of High Availability Disaster Recovery (HADR) systems. We'll cover a few key terms and then some common system topologies.</p>\n<h3 id=\"heading-key-terms-related-to-hadr-systems\">Key terms related to HADR systems</h3>\n<p>Key terms that you should know about HADR systems -</p>\n<h4 id=\"heading-high-availability\">High Availability</h4>\n<p>High availability (HA) describes the ability of an application to withstand all planned and unplanned outages (a planned outage could be performing a system upgrade) and to provide continuous processing for business-critical applications.</p>\n<h4 id=\"heading-disaster-recovery\">Disaster Recovery</h4>\n<p>Disaster recovery (DR) involves a set of policies, tools, and procedures for returning a system, an application, or an entire data center to full operation after a catastrophic interruption. It includes procedures for copying and storing an installed system's essential data in a secure location, and for recovering that data to restore normalcy of operation.</p>\n<h4 id=\"heading-unplanned-downtime\">Unplanned downtime</h4>\n<p>Downtime caused by factors which were not introduced on purpose is called unplanned downtime. This can be majorly due to:</p>\n<ul>\n<li><p>Human Error</p>\n</li>\n<li><p>Software Problems</p>\n</li>\n<li><p>Hardware Failure</p>\n</li>\n<li><p>Environmental Issues</p>\n</li>\n</ul>\n<p><img src=\"https://media.tenor.com/VYujs2dkFTUAAAAC/gopi-bahu.gif\" alt=\"Unplanned downtime meme\" /></p>\n<h4 id=\"heading-planned-downtime\">Planned downtime</h4>\n<p>The opposite of unplanned downtime.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701960127488/0c6eef40-e655-4d53-b6f5-f231a61ca373.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Downtimes introduced on purpose, mostly are:</p>\n<ul>\n<li><p>System upgrades</p>\n</li>\n<li><p>System repairs</p>\n</li>\n<li><p>Restricted access due to business reasons</p>\n</li>\n</ul>\n<h4 id=\"heading-chaos-engineering\">Chaos Engineering</h4>\n<p>Chaos engineering is a method of testing distributed software that deliberately introduces failure and faulty scenarios to verify its resilience in the face of random disruptions. These disruptions can cause applications to respond in unpredictable ways and can break under pressure.</p>\n<h4 id=\"heading-resilience\">Resilience</h4>\n<p>The ability of a solution to absorb the impact of a problem in one or more parts of a system, while continuing to provide an acceptable service level to the business customers.</p>\n<h3 id=\"heading-key-metrics-for-analyzing-your-system-design\">Key metrics for analyzing your system design</h3>\n<p>The key metrics used to analyze system designs are -</p>\n<ul>\n<li><p>Production capacity in and out of region</p>\n</li>\n<li><p>Platform availability</p>\n</li>\n<li><p>Availability during planned outages</p>\n</li>\n<li><p>Failure Impact</p>\n</li>\n<li><p>Disaster recovery time</p>\n</li>\n<li><p>Incident response time</p>\n</li>\n</ul>\n<p>Next, let's look at some HADR system topologies, before we compare their metrics.</p>\n<h3 id=\"heading-a-30000ft-view-of-high-availability-system-design\">A 30,000ft view of high availability system design</h3>\n<p>HADR systems can be designed with several topologies ranging from simple ones - where you put all your eggs in a single basket - or complex ones - where you devise a fail-safe array of servers. Let us study a couple of them to understand how such topologies look like -</p>\n<p>Consider the following 2-Active topology -</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701960139543/0172d1c1-0563-4fa8-9fec-69ddbe54ea48.png\" alt class=\"image--center mx-auto\" /></p>\n<p>This topology shows that we provision 3 servers such that during normal operations, 2 servers load balance the traffic coming to the application while a third server stays on standby. This server gets activated in the event of failure of any or all of the active servers.</p>\n<p>An alternative to a 2-Active system topology is a 3-Active topology -</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701960155075/11f765cb-afec-4340-bd52-7cc5038f7615.png\" alt class=\"image--center mx-auto\" /></p>\n<p>In this system topology, we provision all three servers as active servers and in event of failure of any server, the other servers load balance the traffic, while the failed servers are brought back up.</p>\n<p>An obvious question here - which of these is better?</p>\n<p>Consider the following chart of metrics comparison for the above two systems against a single Active system -</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701960165457/4a97d05e-a089-49de-9bbb-5d5baac74de1.png\" alt class=\"image--center mx-auto\" /></p>\n<p>From the above, it can be said that while 3-active systems gives highest availability and lowest failure impacts, if your application is likely to expect surges, a 2-active system might give you better resilience.</p>\n<p>We shall wrap our discussion about HADR system topologies here. Next, we'll talk about challenges posed by ML in HADR systems and see a demo of these topologies in action!</p>\n<h2 id=\"heading-part-2-hadr-system-challenges-by-ml-deployments-and-load-testing-ml-hadr-system\">Part 2 - HADR system challenges by ML deployments and load testing ML-HADR system</h2>\n<p>Machine Learning heavy deployments bring their own set of challenges to any HADR system. ML models can be deployed in several ways, due to which it becomes important for architects designing HADR systems to choose the right deployment strategies for best results.</p>\n<h3 id=\"heading-challenges-posed-to-ml-pipelines-for-high-availability\">Challenges posed to ML pipelines for high availability</h3>\n<p>Some of the challenges faced with ML deployments for HADR are -</p>\n<h4 id=\"heading-what-is-a-good-ml-deployment\">What is a good ML deployment?</h4>\n<p>The definition of a good ML deployment changes with who is answering this question. Fir examples -</p>\n<p>Business owners - performs fast inference Researchers - highest accuracy Developers - gets built quickly Q/A engineers - never fails Investors - sounds cool, brings in the $$</p>\n<h4 id=\"heading-volume-of-data-processed\">Volume of data processed</h4>\n<p>Volume of data can be a major challenge to most ML systems. Too much data and you may be running late at your training and too less of it, your inference suffers.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701960251922/a7daf4f0-b232-424a-bff0-e65e5f260f04.png\" alt class=\"image--center mx-auto\" /></p>\n<h4 id=\"heading-quality-of-data\">Quality of data</h4>\n<p>Data quality refers to how informative and complete a given chunk of data is. The lower the data quality, the tougher it is to derive insights from it.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701960180925/68483f29-961f-4c7a-92d1-12ef172fcbf0.png\" alt class=\"image--center mx-auto\" /></p>\n<h4 id=\"heading-model-decay\">Model decay</h4>\n<p>The phenomenon in Machine Learning that leads to predictions made by a model become less accurate over time. Primary reasons for model decay are:</p>\n<ul>\n<li><p>Data drift</p>\n</li>\n<li><p>Concept drift</p>\n</li>\n</ul>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701960192366/1a966a6e-e6a5-4a72-9cb3-2f14742d0360.png\" alt class=\"image--center mx-auto\" /></p>\n<h4 id=\"heading-speed-of-inference-against-various-inputs\">Speed of inference against various inputs</h4>\n<p>Speed of inference can change with changing input. If your model performs inference under 1s for most images ranging appx 10MB, what does it do when someone uploads an image of 1GB? Does your system reject the image or does it take down the building?</p>\n<h3 id=\"heading-load-testing-ml-hadr-systems\">Load testing ML-HADR systems</h3>\n<p>Finally, let us load-test a few topologies which server ML based content. To do so, we shall be using the Locus tool along with a self-engineered set of scripts that work as nodes and load balancer.</p>\n<p>I have published the code for this setup here - <a target=\"_blank\" href=\"https://github.com/xprilion/dummy-topology-loadtest\">https://github.com/xprilion/dummy-topology-loadtest</a></p>\n<p>The contents of this system are -</p>\n<ol>\n<li><p><a target=\"_blank\" href=\"http://router.py\"><code>router.py</code></a> : this file will act as a dummy load-balancer.</p>\n</li>\n<li><p><code>predictors/predict**X**.py</code> - these files are numbered, replacing X, and will be active as node servers.</p>\n</li>\n<li><p><code>topology/**topology_name**.json</code> - these json files contain information regarding the topologies available. We will be updating the data inside these files while load testing the topology they represent.</p>\n</li>\n</ol>\n<p>To setup the load test run, first ensure that in the files inside the <code>topology</code> directory, all the topology files are in their initial states, as shown below -</p>\n<h4 id=\"heading-single-server-system\">Single server system</h4>\n<p>In this system, there is a single server handling all resources.</p>\n<pre><code class=\"lang-plaintext\">{\n    \"predict1\": {\n        \"status\": true,\n        \"load\": 0,\n        \"port\": \"9696\"\n    }\n}\n</code></pre>\n<h4 id=\"heading-2-active-system\">2 Active system</h4>\n<p>In this system, initially, there are 2 servers responding to requests. In event of failure of any one or both servers, a third system steps in as replacement while the other two are fixed.</p>\n<pre><code class=\"lang-plaintext\">{\n    \"predict1\": {\n        \"status\": true,\n        \"load\": 0,\n        \"port\": \"9696\"\n    },\n    \"predict2\": {\n        \"status\": true,\n        \"load\": 0,\n        \"port\": \"9697\"\n    },\n    \"predict3\": {\n        \"status\": false,\n        \"load\": 0,\n        \"port\": \"9698\"\n    }\n}\n</code></pre>\n<p>Notice that the <code>status</code> of <code>predict3</code> server is set to <code>false</code>.</p>\n<h4 id=\"heading-3-active-system\">3 Active system</h4>\n<p>In a 3-active system, there are 3 servers available to handle requests.</p>\n<pre><code class=\"lang-plaintext\">{\n    \"predict1\": {\n        \"status\": true,\n        \"load\": 0,\n        \"port\": \"9696\"\n    },\n    \"predict2\": {\n        \"status\": true,\n        \"load\": 0,\n        \"port\": \"9697\"\n    },\n    \"predict3\": {\n        \"status\": true,\n        \"load\": 0,\n        \"port\": \"9698\"\n    }\n}\n</code></pre>\n<p>Then, ensure that in <a target=\"_blank\" href=\"http://router.py\"><code>router.py</code></a>, the <code>topology</code> variable is set to <code>0</code>. This will correspond to the single server system.</p>\n<p>Next, we fire up the Locus UI by running the <code>locust</code> command inside the <code>locust</code> directory.</p>\n<p>Specify the number of users to spawn and the spawn rate. Provide a suitable host. Locust will inform the server under stress that the requests are coming from the specified host.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701960270987/3aeec339-28bc-461c-82a4-952f95fd7c4f.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Click on <strong>Start Swarming</strong> to start throwing requests at the server based on the script specified in the <code>locust/</code><a target=\"_blank\" href=\"http://locustfile.py\"><code>locustfile.py</code></a>.</p>\n<p>Observe the charts that show how the system is responding to the increase in the load. After some time, the first server failures start appearing. We'll keep a note of when the first error appears.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701960281544/39cba6e0-15c5-4b83-ad3d-7f340dace496.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Now, change the <code>topology</code> variable in <a target=\"_blank\" href=\"http://router.py\"><code>router.py</code></a> to <code>1</code> and run Locust. In the next step, change the <code>topology</code> variable to <code>2</code> and run Locust again.</p>\n<p>Let's plot a chart of when the first failures happen in case of each system -</p>\n<div class=\"hn-table\">\n<table>\n<thead>\n<tr>\n<td>Topology</td><td>RPS at failure</td></tr>\n</thead>\n<tbody>\n<tr>\n<td>Single</td><td>5.2</td></tr>\n<tr>\n<td>2-Active</td><td>8.6</td></tr>\n<tr>\n<td>3-Active</td><td>7.4</td></tr>\n</tbody>\n</table>\n</div><p>As expected, the 2-Active system has the peak capacity in our use-case. However, here's an interesting observation -</p>\n<p>Let us compare the values of the predict servers on 2-Active and 3-Active systems after the load test -</p>\n<div class=\"hn-table\">\n<table>\n<thead>\n<tr>\n<td>Server</td><td>2-Active</td><td>3-Active</td></tr>\n</thead>\n<tbody>\n<tr>\n<td>Predict1</td><td>10</td><td>5</td></tr>\n<tr>\n<td>Predict2</td><td>10</td><td>7</td></tr>\n<tr>\n<td>Predict3</td><td>9</td><td>8</td></tr>\n</tbody>\n</table>\n</div><p>As we see, the 2-Active system is completely saturated while the 3-Active system is slightly above half its capacity.</p>\n<p>Thus, even though the 2-Active system fails after the 3-Active system has shown its first error, the 3-Active system will saturate later and continue to serve requests for a longer duration.</p>\n<h2 id=\"heading-conclusion\">Conclusion</h2>\n<p>Depending on the HADR system metric you may want to optimize for, you can choose the topology that works best for your use case. You can choose to have multiple replicas of the same network topology or create your own configuration. The scripts provided for load testing can be extended to more topologies. Have fun testing your HADR system designs with it!</p>\n"},"publishedAt":"2022-10-09T06:30:00.000Z","seo":{"title":"091020221708","description":null},"tags":[{"slug":"blog"},{"slug":"ppt"}]}},{"node":{"id":"6571dae87fa1ee19eb0e5ce6","slug":"divide-vs-unite","url":"https://xprilion.com/divide-vs-unite","title":"Dividing is Easy, Unite if you can","subtitle":null,"brief":"There’s always an easy option and one that needs courage, patience and a strong will.\nWhen I was a kid, every evening, I used to go out to play in a park nearby my home. We were a good mix of kids - boys from several communities and natives of differ...","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/stock/unsplash/tSlvoSZK77c/upload/e0f414854b075ff284b0d0aca39081c1.jpeg"},"content":{"html":"<p>There’s always an easy option and one that needs courage, patience and a strong will.</p>\n<p>When I was a kid, every evening, I used to go out to play in a park nearby my home. We were a good mix of kids - boys from several communities and natives of different states. One day, two new kids showed up and asked us if they could play with us. What was unusual about them was that unlike us, they wore shabby clothes, looked malnourished and were visibly shy. For a group of kids who had known each other for years and were mostly from decent households, we had been trained since childhood to avoid playing with anyone who resembled these two newcomers.</p>\n<p>But did we care? Absolutely not. We played together and didn’t give any damn to any of the differentiating factors. We were kids playing in a park and that was our identity. Till this point in this story, I believe I have everyone’s approval. “You did good”, “kids are so pure” and all that feel-good responses are probably what I would get if I ended it here.</p>\n<p>But that would be the easy option. From this point onwards, you will start categorizing me according to your checklist of right and wrong.</p>\n<p>The two kids were from a small community of migrants from Bangladesh and their father was one of the rare few from that community who managed to break free from crushing poverty by landing a good job. If you’re guessing their religion, yes, they were not Hindus, unlike the rest of our group.</p>\n<p>And now, if you count all the differences we had between those two kids and the rest of the group - you’ll realize that differentiating is easy. It is easy to break down people into small groups. We all carry multiple identities all our lives - our name, the place we were born in, the language we speak, the job we do, the religion we practice - just so that people can differentiate us.</p>\n<p>One could argue - all the factors I mentioned above are more about uniting people under a banner than dividing them. And that is true. The smallest and the largest wars in history have been fought only by uniting people under at least two banners. The World Wars saw multiple nations uniting into groups differentiated by ideologies. The world has seen genocides and prolonged oppressions done by differentiating people and uniting the group of people with the oppressing mindset.</p>\n<p>What I wish to convey from the above set of examples is that every time we unite people after dividing them on any basis - we create a separation that people will protect, be jealous about and fight for. Not because they’re wrong - because they believe in the identity that separation provides them. Each person will guard that identity - some with words, some with art and some with arms. Wars will always be fought with all three - words, art and arms.</p>\n<p>But let me not stray into the philosophy of wars and what’s wrong and wrong with them. Coming back to divides and unifications. Let’s look at the tougher option between the two - uniting people.</p>\n<p>Why is uniting people the tougher option? Because it needs the courage to ask people to let go of identity, the patience to deal with the differences within the group and a strong will that needs to be instilled in each member of the group to want to be in that group, letting go of all the factors that can be used to feel different from the rest. We all, as individuals, strive to be unique, to be recognized separately and to be rewarded separately.</p>\n<p>History tells us that every time a major achievement was made by a group of people - it was only after they reduced differences that broke them into smaller groups, came together and united to achieve their goal. These smaller groups had to come together, forgetting the small battles they have had with each other and form that larger group that moved the mountain. They had to learn to respect the differences of the other and find the unifying factor that they were more passionate about than the dividing factors. Every individual in that large group had to give up their most differentiating identities and associate themselves with a more general identity.</p>\n<p>You might think that uniting is what leaders do best. True, that’s why they’re the leaders. Would be fancy to see a leader who doesn’t have a group of people united to follow him. Here’s a question though - are politicians leaders? My answer to it - sometimes. Not all leaders are politicians and not all politicians are leaders. While you may argue that both bring people together to do their bidding - either to achieve a goal or to get a vote, I differentiate them as such - leaders wish to unite people by letting go of differences while a politician tries to unite people by creating divides. A leader takes the tough option, a politician takes the easy one. What about leaders who were also politicians? We are not exactly in the age where leaders just raised an army and got things done.</p>\n<p>Uniting is tough. Letting go of differences is tough. Accepting the other is tough. <strong>You have to be jealously human to not care about any other factor.</strong></p>\n<p>If you've reached this point - congratulations! You and I are likely on the same side of the ideological table. If not, feel free to bash me in the comments or think about all that I said. The former is the easy option and the latter being tough, will take your time and effort to constructively argue against my points. The choice lies with you :)</p>\n"},"publishedAt":"2022-03-18T06:30:00.000Z","seo":{"title":"180320221708","description":null},"tags":[{"slug":"blog"}]}},{"node":{"id":"6571ce52473e6affa1acac50","slug":"cost-of-standing-out","url":"https://xprilion.com/cost-of-standing-out","title":"The Cost of Standing Out","subtitle":null,"brief":"9 years back, on this date, I launched a social network built from scratch. Almost a month later, I added on the same platform a search engine, again, built from scratch.\nThe website gained almost 60,000 users and was soon among the top 1000 websites...","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/stock/unsplash/oMpAz-DN-9I/upload/4fbbce188e19b64a73cdfc9719c58bf2.jpeg"},"content":{"html":"<p>9 years back, on this date, I launched a social network built from scratch. Almost a month later, I added on the same platform a search engine, again, built from scratch.</p>\n<p>The website gained almost 60,000 users and was soon among the top 1000 websites in India by Alexa traffic rankings (in 2013). Little did I know the value of what I had created back then. Around 1.5 years later, I shut it down to focus on my studies (11th grade).</p>\n<p>While I do regret that I did not have anyone to give me better advice back then, this article touches on a different issue.</p>\n<p>I recall, clearly, that several posts on the social network by some of my schoolmates would troll me, my father (who was a teacher in the school I studied in) and my sister. Many of them on the search engine would search our names accompanied by slangs and abuses.</p>\n<p>I failed to understand why it happened, and I took the pain of seeing this silently, without sharing it with anyone. Yes, we were all too young then to understand cyber-bullying. But were we too young to not feel the pain it caused? And how do you explain what he did wrong to a kid who put in all his heart and soul in creating a platform, which not many at his age could do, which was used to abuse him?</p>\n<p>9 years later, I realize that what those schoolmates did was not because of their hatred towards me, but because they could not digest the fact that someone from among them was trying to stand out of the norm. God bless the rare few of them who found success in their passions, most of those schoolmates today are toiling the 9-5 at companies that will replace them at the first opportunity.</p>\n<p>I do not name anyone here, my intention is not to hold anyone accountable. The message that I do want to pass is - <strong>the next time a kid comes up to you with whatever they've created, could be a software, could be a painting or could simply be a cute shape drawn on sand, have the heart to let them have their moment.</strong></p>\n<p>Reflecting, I believe that the experience I gained from building the platform back then still holds for me insights I often draw upon. Even today, that kid 9 years back motivates me to not give a single fuck to anyone who tries to hold me back from standing out.</p>\n"},"publishedAt":"2021-05-26T06:30:00.000Z","seo":{"title":"260520211229","description":null},"tags":[{"slug":"blog"}]}},{"node":{"id":"6571cfd70a3841655801258e","slug":"decoding-propaganda-in-memes","url":"https://xprilion.com/decoding-propaganda-in-memes","title":"Decoding Propaganda In Memes","subtitle":null,"brief":"Memes - the nice images you see floating around literally everywhere on the Internet these days and the stuff that fuels your social media. Memes began as a source of entertainment - containing jokes and visual additions to the joke which would often...","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/stock/unsplash/Tq7lbAeF9BQ/upload/73664e85021ea21db0dec23e64b8b65e.jpeg"},"content":{"html":"<p>Memes - the nice images you see floating around literally everywhere on the Internet these days and the stuff that fuels your social media. Memes began as a source of entertainment - containing jokes and visual additions to the joke which would often add more spice to it. However, memes have evolved in the recent times to deliver a wider range of content - news, opinions, dissent, praises and unfortunately - propaganda and fake information.</p>\n<p>What goes on in your mind when you view a meme which is funny, but is under the hood demeaning to a community or a group of people who believe in something? Have you seen memes which you felt were trying hard to push the ideology of a certain section of people in the society or memes which felt downright an attack on someone?</p>\n<p>Are you able to recognize such memes instantly? Or has it been that you shared a meme which you found funny but someone pointed out its falsehood to you, only after you had already spread it to a good number of people? It is important here to realize that opinion and falsehood are different things. It is crucial to be able to differentiate when you are spreading ideas and when you are being a tool for spreading misinformation by sharing a meme which has a falsehood based bias in it.</p>\n<p>Let's try to understand the width of the impact that biased memes can have on your day to day life. Consider the following Google Trends chart about how much \"political meme\" has been searched in the last decade (October 2010 - October 2020) -</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701957423879/a95e4fa3-0c5d-4256-adc0-6a832e0c206f.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Notice the days when there has been a major spike. You shall observe the spikes happened in the years - 2020, 2016, 2012. What's common in these years? The US Presidential elections.</p>\n<p>This article, is going to help you with a few pointers which you can keep in mind to realize the bias in memes that you view, in a hope that the next time you share a meme, you'll have a fair idea about what impact it can have.</p>\n<p>Case #1 - The Positive Propaganda</p>\n<p>Let's begin on a lighter tone. Propaganda need not always be negative. It could be directed towards creating a genuine positive sentiment for someone/something. Take a look at this one -</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701957495306/1d29c1e2-5922-4d0d-9059-5f14879a457b.png\" alt class=\"image--center mx-auto\" /></p>\n<p>This one is fairly easy to understand - a happy man. Happiness radiates the presence of no worries and thus, a content person. Now, you might ask yourself who do you see as content - someone who has made good decisions in life? Or is it someone who has a lot of money and does not have to worry about the trivials of life? In short, someone who is successful.</p>\n<p>Thus, if you were to see this meme anywhere, you would take the presented political leader as someone who is successful and does not worry about his opponents.</p>\n<p>In contrast, if I were to present you with the following meme -</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701957513788/ba808ed3-e317-4638-915f-20ddd15b9912.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Here you find that the same political leader is asking for financial support for his campaigns. While this is a serious and obvious message, it is seen as a moment of weakness. Thus, it was imperative for the opponents to seize opportunity of this moment and turn it into a popular meme, such that every time someone saw this image of the political leader, they would develop a feeling that the leader was weak and would not likely be successful in his campaign.</p>\n<p>Case #2 - Destroying image of organization/person</p>\n<p>Often, memes target a single person or organization. This may be done for several reasons, but often it has happened that this leads to media trials and unfair pressure against an entity which may not have done anything wrong.</p>\n<p>Consider the following meme -</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701957525960/4c7458cf-8dfc-49b1-885c-e040bf4e63c4.png\" alt class=\"image--center mx-auto\" /></p>\n<p>While it may tickle the bone of some for the implications that the teacher in the meme is actually a terrorist trying to educate the student in terrorist activities, there is an addition of the logo of Khan Academy at 3 places in this meme. This has been done to implicate that Khan Academy is a terrorist organization and the education imparted there is to train terrorists.</p>\n<p>To an uninformed individual, this meme holds the potential to set a negative image of Khan Academy in his/her mind. Further, the meme tries to implicate that any organization starting with the word \"Khan\" may be a terrorist organization and furthers this point by adding a Muslim skull cap to the student in the meme.</p>\n<p>For those of you who do not know - Khan Academy is one of the most revolutionary ideas that came up in this century and has benefited students all across the globe by offering free education. Several other educational startups sprang up trying to replicate their model with a difference that they would charge money. Khan Academy remains free and unparalleled to the date.</p>\n<p>It is uncomely that such attempt at maligning a beautiful initiative has been made.</p>\n<p>Case #3 - The Hate spreading</p>\n<p>Now let's look at some more serious type of biases memes can present. Spreading hate is one of the most common acts that memes try to pull of these days. Look at the following memes -</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701957551395/36ebb6f6-ee89-4194-9b52-e8642be9ec1b.png\" alt class=\"image--center mx-auto\" /></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701957560571/35614c25-5416-41a3-8b16-83546bc4e9f9.png\" alt class=\"image--center mx-auto\" /></p>\n<p>In both the above instances, hatred is being spread either in the meme itself or by the caption. The first image is an instance of spreading hatred towards an entire community and further, in the caption it uses a popular provoking slogan which was used by certain political leaders to incite venom towards a larger community in a different situation.</p>\n<p>The other one is a morphed image of a political leader aiming to indignify the individual and thus, weaken the person's worth in the society.</p>\n<p>Such memes which openly target a certain person or society either within the meme itself or by the accompanying caption tend to be repeatedly promoted by people of the group which benefits by sharing them. Such groups of people who want those memes to become viral will likely post it several times in the same or similar forums and try to use more and more provocative language along with them to incite hot reactions from the opposing groups, thus increasing the reach of their posts.</p>\n<h2 id=\"heading-the-checklist-of-propaganda-memes\">The Checklist of Propaganda Memes</h2>\n<p>Now let me get down to a checklist which can help you identify quickly memes which are trying hard to send down your brain propaganda of their benefit -</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701957581385/63d6f2fb-1d96-4b2a-9627-fe5a5145f93c.png\" alt class=\"image--center mx-auto\" /></p>\n<p>If the meme you are viewing ticks any of the above points, you might want to double check it for propaganda. It may not always be the same, but in most cases you will find evidence of it promoting or discrediting a certain idea, person or community.</p>\n<p>Pages or social media influencers who regularly post such content might be trying to politically or socially influence you into hating or promoting a single idea, political or social figure or community. It might be a wise idea to give such pages an unfollow to make sure that you base your opinions on factual information instead of the idea that is fed to you subconsciously through online propaganda campaigns which in the current date often include memes, popular meme pages, influencers, videos which may seem humorous, etc.</p>\n<p>Remember that your memes have to be entertaining - if it is not funny, it is not a meme, its plain propaganda. Don't let anyone buy their way into your mental health and influence you to hate someone.</p>\n<p>Be safe online!</p>\n<p>If you have instances of memes which are biased or drive a propaganda, feel free to put them in the comments below with what you think it is trying to do. It would an added case study and definitely help the next reader! :)</p>\n"},"publishedAt":"2020-10-11T06:30:00.000Z","seo":{"title":"111020201229","description":null},"tags":[{"slug":"blog"}]}},{"node":{"id":"6571d93411863a3420acc692","slug":"python-amazon-keyspaces-cassandra","url":"https://xprilion.com/python-amazon-keyspaces-cassandra","title":"Working with Amazon Keyspaces Cassandra distribution using Python","subtitle":null,"brief":"Cassandra is a popular NoSQL database with capabilities to handle massive data by using a distributed array of commodity hardware. After a boring introduction, here's the fun part - Cassandra is a Facebook contribution to the world of Open Source, ha...","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1701959885282/69e22132-ac0b-4689-8725-06cfd9f31102.png"},"content":{"html":"<p><a target=\"_blank\" href=\"https://cassandra.apache.org/doc/latest/\">Cassandra</a> is a popular NoSQL database with capabilities to handle massive data by using a distributed array of commodity hardware. After a boring introduction, here's the fun part - Cassandra is a Facebook contribution to the world of Open Source, having been developed to handle the inbox search feature in Facebook almost a decade ago, and since it has been handed over to the <a target=\"_blank\" href=\"https://www.apache.org/\">Apache Software Foundation</a>, it has been among the top open source projects under the organization.</p>\n<p>Cassandra powers the highly intensive queries in the applications of several major tech players - Instagram, Netflix, Facebook (before replacing it with HBase and further HBase with MyRocks), Twitter (before replacing it with in-house Manhattan), Walmart Labs, CERN, Cisco WebEx, and many more. You can read an extensive praise-o-logy of Cassandra on this <a target=\"_blank\" href=\"https://ubuntu.com/blog/apache-cassandra-top-benefits\">Ubuntu blog titled What is Cassandra and why are big tech companies using it?</a></p>\n<p>While it can be a hassle to install and maintain a Cassandra database server online, <a target=\"_blank\" href=\"https://aws.amazon.com/keyspaces/\">Amazon Keyspaces</a> offering by Amazon Web Services makes it a no-brainer to use. A similar powerful offering is provided by Datastax by the name <a target=\"_blank\" href=\"https://www.datastax.com/products/datastax-astra\">Astra</a>. Both provide a managed service for Cassandra database which you can readily use both for development and production needs. Big plus - you can try both for free!</p>\n<p>In this tutorial, I shall be moving ahead with Amazon Keyspaces, considering the dearth of a complete example of how to use it on the Internet. You can very easily modify it to work with Datastax Astra.</p>\n<p>First off, before we make any queries, head over to the <a target=\"_blank\" href=\"https://console.aws.amazon.com/keyspaces/home\">Amazon Keyspaces dashboard</a> and create a Keyspace and a sample Table using the Dashboard or the CQL Editor. Both actions being fairly simple, I shall leave it up to you to <a target=\"_blank\" href=\"https://docs.aws.amazon.com/keyspaces/latest/devguide/getting-started.ddl.html\">explore how to get it done</a>. Let's move ahead with how you can use them with a Python application.</p>\n<p>For convenience, I shall assume that the keyspace you created is named <code>test_keyspace</code> and the table is named <code>users</code>. The definition for <code>users</code> table is expected to be -</p>\n<div class=\"hn-table\">\n<table>\n<thead>\n<tr>\n<td>Column</td><td>Type</td></tr>\n</thead>\n<tbody>\n<tr>\n<td>id</td><td>uuid</td></tr>\n<tr>\n<td>name</td><td>varchar</td></tr>\n<tr>\n<td>age</td><td>integer</td></tr>\n<tr>\n<td>city</td><td>varchar</td></tr>\n</tbody>\n</table>\n</div><p>You'll need to download a <a target=\"_blank\" href=\"https://www.comodo.com/resources/small-business/digital-certificates.php\">Digital Certificate</a> provided by Amazon to be able to connect to Keyspaces since the service only connects through TLS. To do so, use the following command in a terminal window -</p>\n<pre><code class=\"lang-bash\">curl https://www.amazontrust.com/repository/AmazonRootCA1.pem -O\n</code></pre>\n<p>Create a new folder in your working directory named <code>.cassandra</code> and move the <code>AmazonRootCA1.pem</code> file there. Just cleaning up the working space, tbh, you can keep it wherever you wish, as long as its accessible to your Python script.</p>\n<p>Next, we shall need the <code>cassandra-driver</code> library for quick functionality to use Cassandra with Python. Run the following command to install it -</p>\n<pre><code class=\"lang-bash\">pip install cassandra-driver\n</code></pre>\n<p>Now, we shall write a small barebones wrapper object for our connection with Amazon Keyspaces. Create a file called <a target=\"_blank\" href=\"http://db.py\"><code>db.py</code></a> in your working directory.</p>\n<p>Add the following code to make all necessary imports -</p>\n<pre><code class=\"lang-python\"><span class=\"hljs-keyword\">from</span> cassandra.auth <span class=\"hljs-keyword\">import</span> PlainTextAuthProvider\n<span class=\"hljs-keyword\">from</span> cassandra.cluster <span class=\"hljs-keyword\">import</span> Cluster\n<span class=\"hljs-keyword\">from</span> ssl <span class=\"hljs-keyword\">import</span> SSLContext, PROTOCOL_TLSv1_2, CERT_REQUIRED\n<span class=\"hljs-keyword\">from</span> cassandra <span class=\"hljs-keyword\">import</span> ConsistencyLevel\n<span class=\"hljs-keyword\">from</span> cassandra.query <span class=\"hljs-keyword\">import</span> SimpleStatement\n</code></pre>\n<p>The above imports indicate a few things -</p>\n<ul>\n<li><ol>\n<li>We're going to be using the <code>PlainTextAuthProvider</code> which means at some point we'll need a username and password combination for our connection with the database. We shall come around this.</li>\n</ol>\n</li>\n<li><ol>\n<li><code>Cluster</code> indicates that we shall be creating an object of the Cassandra Cluster we connect to. This provides cluster level operations which can be useful for you, but not much in the course of this tutorial.</li>\n</ol>\n</li>\n<li><ol>\n<li><code>SSLContext, PROTOCOL_TLSv1_2, CERT_REQUIRED</code> are all required for making a TLS connection to the Amazon Keyspaces service.</li>\n</ol>\n</li>\n<li><ol>\n<li><code>ConsistencyLevel</code> if this is a bouncer, we shall get to overcoming it further down in this tutorial.</li>\n</ol>\n</li>\n<li><ol>\n<li><code>SimpleStatement</code> is a simple CSQL statement, no wonders here.</li>\n</ol>\n</li>\n</ul>\n<p>Next, let's begin using these imports.</p>\n<p>Create a class <code>Cassandra</code> and add an initialization constructor which creates and connection to the database service -</p>\n<pre><code class=\"lang-python\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">Cassandra</span>:</span>\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">self</span>):</span>\n        ssl_context = SSLContext(PROTOCOL_TLSv1_2)\n        ssl_context.load_verify_locations(<span class=\"hljs-string\">'.cassandra/AmazonRootCA1.pem'</span>)\n        ssl_context.verify_mode = CERT_REQUIRED\n        auth_provider = PlainTextAuthProvider(\n            username=<span class=\"hljs-string\">'ServiceUsername'</span>,\n            password=<span class=\"hljs-string\">'ServicePassword'</span>)\n        self.cluster = Cluster(\n            [<span class=\"hljs-string\">'cassandra.us-east-1.amazonaws.com'</span>],\n            ssl_context=ssl_context,\n            auth_provider=auth_provider,\n            port=<span class=\"hljs-number\">9142</span>)\n        self.session = self.cluster.connect(<span class=\"hljs-string\">\"test_keyspace\"</span>)\n</code></pre>\n<p>Notice <code>ServiceUsername</code> and <code>ServicePassword</code>, you do not yet have them. To create a pair of credentials to use, follow this instructions in <a target=\"_blank\" href=\"https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.credentials.html#programmatic.credentials.ssc\">this tutorial</a>.</p>\n<p>Your Cluster endpoint (<a target=\"_blank\" href=\"http://cassandra.us-east-1.amazonaws.com\"><code>cassandra.us-east-1.amazonaws.com</code></a>) could differ from the one I have used in my example. You can find out your endpoint by visiting <a target=\"_blank\" href=\"https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.endpoints.html\">this list</a> and use the endpoint corresponding to your AWS Region.</p>\n<p>Now, let's create a method for the <code>Cassandra</code> class that we can use to execute queries -</p>\n<pre><code class=\"lang-python\"><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">Cassandra</span>:</span>\n    ...\n    <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">execute</span>(<span class=\"hljs-params\">self, query</span>):</span>\n            <span class=\"hljs-keyword\">return</span> self.session.execute(SimpleStatement(\n                query, consistency_level=ConsistencyLevel.LOCAL_QUORUM))\n</code></pre>\n<p>Look, we used the <code>ConsistencyLevel</code> object here! What is this bird?</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701959976438/66abf0eb-5036-4fdf-86c2-3ec3467876db.jpeg\" alt class=\"image--center mx-auto\" /></p>\n<p><strong>Consistency Level</strong> is the number of nodes which need to confirm that a particular write operation into a Cassandra database is successful. In simpler words, since Cassandra is a distributed database, and the data stored in Cassandra is split over multiple nodes, a single write operation is successful only when maximum possible nodes acknowledge the write operation as valid. For this, we use different consistency levels, some of which are - <code>LOCAL_ONE</code>, <code>LOCAL_QUORUM</code>, <code>ALL</code>, etc. You can read more about consistency levels in <a target=\"_blank\" href=\"https://www.geeksforgeeks.org/consistency-levels-in-cassandra/\">this blog by Ashish Rana on GeeksforGeeks</a>.</p>\n<p>Currently, Amazon Keyspaces works only with the <code>LOCAL_QUORUM</code> consistency level.</p>\n<p>Now, we can delve into some action!</p>\n<p>Create a new file called <a target=\"_blank\" href=\"http://main.py\"><code>main.py</code></a> (or whatever fancy wording you can think of in 1 second), and put the following lines in it to try inserting a new entry to the database -</p>\n<pre><code class=\"lang-python\"><span class=\"hljs-keyword\">from</span> db <span class=\"hljs-keyword\">import</span> Cassandra\n\ncsql = Cassandra()\n\n<span class=\"hljs-comment\"># Insert Query</span>\nresults = csql.execute(<span class=\"hljs-string\">\"INSERT INTO users (id, name, age, city) \\\n                        VALUES (6ab09bec-e68e-48d9-a5f8-97e6fb4c9b47, \\\n                       'John', 24, 'Delhi')\"</span>)\n</code></pre>\n<p>Now, save the file and try executing it from the terminal using the following command -</p>\n<pre><code class=\"lang-bash\">python main.py\n</code></pre>\n<p>If the query is successful, you shall see no errors.</p>\n<p>Let's try reading the database and see if we've got the entry right. Comment out the Insert query code in the <a target=\"_blank\" href=\"http://main.py\"><code>main.py</code></a> file and add the following lines -</p>\n<pre><code class=\"lang-python\"><span class=\"hljs-comment\"># Read query</span>\nresults = csql.execute(<span class=\"hljs-string\">\"SELECT * FROM users\"</span>)\nprint([x <span class=\"hljs-keyword\">for</span> x <span class=\"hljs-keyword\">in</span> results])\n</code></pre>\n<p>You should see the output similar to this -</p>\n<pre><code class=\"lang-text\">[Row(id=UUID('6ab09bec-e68e-48d9-a5f8-97e6fb4c9b47'), city='Delhi', age=24, name='John')]\n</code></pre>\n<p>If not, you need to observe the error message and try fixing the code! You can find the full code for this tutorial at - <a target=\"_blank\" href=\"https://github.com/xprilion/python-amazon-keyspaces\">https://github.com/xprilion/python-amazon-keyspaces</a></p>\n<p>Cassandra can be a great tool if you're looking to build highly scalable and mission-critical applications (given you need something like Cassandra, at all) and Amazon Keyspaces makes it very simple and efficient to use and manage.</p>\n<p>It is possible to use Amazon Keyspaces with other backends as well, and I shall leave it for you to explore them at your interest.</p>\n<p>Thanks for reading this!</p>\n"},"publishedAt":"2020-09-17T06:30:00.000Z","seo":{"title":"100009","description":null},"tags":[{"slug":"blog"}]}},{"node":{"id":"6571d85a77d326ef2b67bc50","slug":"generic-mongodb-wrapper-using-flask-pymongo","url":"https://xprilion.com/generic-mongodb-wrapper-using-flask-pymongo","title":"A Generic MongoDB Wrapper API with Flask and PyMongo","subtitle":null,"brief":"Hey there, hope you're well! It's been a while since I wrote something here, to my defence I wrote this tutorial about How to setup a secure, remote JupyterLab workstation on DigitalOcean. Have you not read it yet? Go ahead and explore it if you're i...","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/stock/unsplash/cijiWIwsMB8/upload/1555ce1e680abb4a372de2e2e473f3da.jpeg"},"content":{"html":"<hr />\n<p>Hey there, hope you're well! It's been a while since I wrote something here, to my defence I wrote this tutorial about <a target=\"_blank\" href=\"https://www.digitalocean.com/community/tutorials/how-to-set-up-a-jupyterlab-environment-on-ubuntu-18-04\">How to setup a secure, remote JupyterLab workstation</a> on <a target=\"_blank\" href=\"https://digitalocean.com\">DigitalOcean</a>. Have you not read it yet? Go ahead and explore it if you're interested in the topic! Now, moving ahead.</p>\n<p>This blog is going to be about creating a generic wrapper API for your MongoDB installation using Flask and PyMongo. If you've little idea about what these terms mean, here we go -</p>\n<h2 id=\"heading-1-pymongo-and-mongodb\">1. PyMongo and MongoDB</h2>\n<p>When you're working with the cool <a target=\"_blank\" href=\"https://mongodb.com\">MongoDB</a> using Python, <a target=\"_blank\" href=\"https://pymongo.readthedocs.io/en/stable/\">PyMongo</a> is your go to tool. The distribution facilitates a full fledged support for interacting with MongoDB databases, local or remote and provides a dead simple way of working with it through your Python code.</p>\n<h2 id=\"heading-2-flask\">2. Flask</h2>\n<p>A very popular library in Python for creating web sites, often preferred for lightweight tasks and more often for creating API servers quickly, <a target=\"_blank\" href=\"https://flask.palletsprojects.com\">Flask</a> provides a way to have a no-frills web server running in minutes.</p>\n<h2 id=\"heading-3-you\">3. You</h2>\n<p><img src=\"https://media1.tenor.com/images/07feba4572471b21fd4258b6af83c9c4/tenor.gif\" alt=\"Who's awesome?\" /></p>\n<p>A super cool person reading this nice blog for whom I hope I can make the time spent here worth!</p>\n<p>Now that we're done with the introductions, let's chalk out a plan that we shall follow in this tutorial. Imagine a situation where you have a MongoDB database with yourselves, and are wondering how to use it for some basic <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Create,_read,_update_and_delete\">CRUD</a> operations from a remotely running application. Further, to make things interesting, you're unsure what collections in the database your application shall need, or it may even need to create arbitrary collections at any moment. Fortunately, you hit upon the golden idea of creating some public APIs to your database server using Flask which allow just those operations with our without some authentication, as per your need.</p>\n<p>While I'm going to leave the (tiny) headache of getting a MongoDB server to you, which you can easily get on many online providers, even for free at some places, I shall begin with the requirements of our API.</p>\n<h2 id=\"heading-the-specification\">The specification</h2>\n<p>If you went through the specification of CRUD, you know that we need 4 REST APIs to begin with -</p>\n<div class=\"hn-table\">\n<table>\n<thead>\n<tr>\n<td>Action</td><td>Method</td><td>MongoDB</td><td>Parameters</td><td>Endpoint</td></tr>\n</thead>\n<tbody>\n<tr>\n<td>Create</td><td>POST, PUT</td><td>Insert</td><td>None</td><td><a target=\"_blank\" href=\"http://example.com/table\">example.com/table</a></td></tr>\n<tr>\n<td>Read All/One</td><td>GET</td><td>Find</td><td>None/Resource ID</td><td><a target=\"_blank\" href=\"http://example.com/table/[id]\">example.com/table/[id]</a></td></tr>\n<tr>\n<td>Update</td><td>POST, PUT, PATCH</td><td>Update</td><td>Resource ID</td><td><a target=\"_blank\" href=\"http://example.com/table/id\">example.com/table/id</a></td></tr>\n<tr>\n<td>Delete</td><td>DELETE</td><td>Delete</td><td>Resource ID</td><td><a target=\"_blank\" href=\"http://example.com/table/id\">example.com/table/id</a></td></tr>\n</tbody>\n</table>\n</div><h2 id=\"heading-the-preparation\">The preparation</h2>\n<p>Now, we shall make sure we have some helper code ready for usage, usually to handle any errors on the API or simply to respond to an \"are you alive\" ping from anywhere remote.</p>\n<p>First off, you'd need the right libraries installed. We shall be using the <code>flask_pymongo</code> which is a wrapper around PyMongo with helpers for Flask provided. Besides, we will need the <code>flask_cors</code> library to <a target=\"_blank\" href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS\">allow fetching of resources from a different domain</a>.</p>\n<pre><code class=\"lang-bash\">pip install flask, flask_pymongo, flask_cors\n</code></pre>\n<p>Let's import these libraries (and others which might be needed) into our script.</p>\n<h3 id=\"heading-step-1-importing-libraries\">Step 1: Importing libraries</h3>\n<p>Create a file named <a target=\"_blank\" href=\"http://app.py\"><code>app.py</code></a> in your working directory. Add the following lines to import all needed libraries -</p>\n<pre><code class=\"lang-python\"><span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\">from</span> flask <span class=\"hljs-keyword\">import</span> Flask, request, make_response\n<span class=\"hljs-keyword\">from</span> flask_pymongo <span class=\"hljs-keyword\">import</span> PyMongo, ObjectId\n<span class=\"hljs-keyword\">from</span> flask_cors <span class=\"hljs-keyword\">import</span> CORS\n<span class=\"hljs-keyword\">import</span> datetime\n<span class=\"hljs-keyword\">from</span> bson.json_util <span class=\"hljs-keyword\">import</span> dumps\n</code></pre>\n<h3 id=\"heading-step-2-setup-constants-mongodb-connection-and-helper-objects\">Step 2: Setup constants, MongoDB connection and helper objects</h3>\n<p>Now, let us define the stuff we shall be needing globally in the code.</p>\n<p>First, let's create an instance of our Flask app -</p>\n<pre><code class=\"lang-python\">app = Flask(__name__)\nCORS(app)\nAPP_ROOT = os.path.dirname(os.path.abspath(__file__))\n</code></pre>\n<p>The <code>CORS(app)</code> enables CORS for the API we shall run. Next, let's define some HTTP status code responses we shall be making in the API.</p>\n<pre><code class=\"lang-python\"><span class=\"hljs-comment\"># HTTP status code constants</span>\nHTTP_SUCCESS_GET_OR_UPDATE          =   <span class=\"hljs-number\">200</span>\nHTTP_SUCCESS_CREATED                =   <span class=\"hljs-number\">201</span>\nHTTP_SUCCESS_DELETED                =   <span class=\"hljs-number\">204</span>\nHTTP_SERVER_ERROR                   =   <span class=\"hljs-number\">500</span>\nHTTP_NOT_FOUND                      =   <span class=\"hljs-number\">404</span>\nHTTP_BAD_REQUEST                    =   <span class=\"hljs-number\">400</span>\n</code></pre>\n<p>Then, create a connection object for PyMongo -</p>\n<pre><code class=\"lang-python\">app.config[<span class=\"hljs-string\">\"MONGO_URI\"</span>] = <span class=\"hljs-string\">\"mongodb://username:password@host:port/database?authSource=admin\"</span>\nmongo = PyMongo(app)\n</code></pre>\n<p>Finally, we shall be creating a helper function that prepares the responses our API makes -</p>\n<pre><code class=\"lang-python\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">send</span>(<span class=\"hljs-params\">data, status_code</span>):</span>\n    <span class=\"hljs-keyword\">return</span> make_response(dumps(data), status_code)\n</code></pre>\n<p>This function will be responsible for preparing the data before finally giving it out through the API. Sometimes you might want global transforms on all output you're producing, this is where they go.</p>\n<h2 id=\"heading-the-real-deal\">The real deal</h2>\n<p>Now, we can setup the APIs to wrap around PyMongo. Let us start with the <code>Create</code> API.</p>\n<h3 id=\"heading-step-4-create-api\">Step 4: Create API</h3>\n<p>In this step, I shall first put out the code for you to study, and then attempt explaining it.</p>\n<pre><code class=\"lang-python\"><span class=\"hljs-meta\">@app.route('/&lt;collection_name&gt;', methods=['POST'])</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">post_item</span>(<span class=\"hljs-params\">collection_name</span>):</span>\n    <span class=\"hljs-string\">\"\"\"\n        Post one item in collection.\n    \"\"\"</span>\n    collection = getattr(mongo.db, collection_name)\n    formdata = request.json\n    <span class=\"hljs-keyword\">try</span>:\n        insert_id = str(collection.insert_one(formdata).inserted_id)\n        output = {<span class=\"hljs-string\">'message'</span>: <span class=\"hljs-string\">'new item created'</span>, <span class=\"hljs-string\">\"_id\"</span>: insert_id}\n        <span class=\"hljs-keyword\">return</span> send(output, HTTP_SUCCESS_CREATED)\n    <span class=\"hljs-keyword\">except</span> Exception <span class=\"hljs-keyword\">as</span> e:\n        output = {<span class=\"hljs-string\">'error'</span> : str(e)}\n        <span class=\"hljs-keyword\">return</span> send(output, HTTP_BAD_REQUEST)\n</code></pre>\n<p>In the code above, you see that we created a <code>/&lt;collection_name&gt;</code> route. This means that we have kept the collection name generic, and whatever collection name is being passed to it, the API will attempt to insert data into it, or if it doesn't exist yet in the database, it will first create the collection and then insert the item.</p>\n<p>The API returns an <code>id</code> which is useful for entry level operations like reading a single entry, updating it or deleting it.</p>\n<h3 id=\"heading-step-5-read-api\">Step 5: Read API</h3>\n<p>Next, we shall create an API to read the entries in the collection. There can be two instances of read - one in which you try to read exactly 1 entry and the other where you need all entries. Let us create these.</p>\n<p>First, let's create an API that returns all items in a collection -</p>\n<pre><code class=\"lang-python\"><span class=\"hljs-meta\">@app.route('/&lt;collection_name&gt;', methods=['GET'])</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">get_all_items</span>(<span class=\"hljs-params\">collection_name</span>):</span>\n    <span class=\"hljs-string\">\"\"\"\n        Documents in a collection.\n    \"\"\"</span>\n    collection = getattr(mongo.db, collection_name)\n    output = []\n    <span class=\"hljs-keyword\">for</span> q <span class=\"hljs-keyword\">in</span> collection.find():\n        output.append(q)\n    <span class=\"hljs-keyword\">return</span> send(output, HTTP_SUCCESS_GET_OR_UPDATE)\n</code></pre>\n<p>Now, any <code>GET</code> request to any <code>collection_name</code> will attempt to display all items in that collection. If it does not exist, the API will simply return a blank list, but will not attempt to create the collection.</p>\n<p>Then, let's create the route to get information about a single entry -</p>\n<pre><code class=\"lang-python\"><span class=\"hljs-meta\">@app.route('/&lt;collection_name&gt;/&lt;id&gt;', methods=['GET'])</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">get_one_item</span>(<span class=\"hljs-params\">collection_name, id</span>):</span>\n    <span class=\"hljs-string\">\"\"\"\n        Get one item from a collection.\n    \"\"\"</span>\n    collection = getattr(mongo.db, collection_name)\n    r = collection.find_one({<span class=\"hljs-string\">'_id'</span>: ObjectId(id)})\n    <span class=\"hljs-keyword\">if</span> r:\n        <span class=\"hljs-keyword\">return</span> send(r, HTTP_SUCCESS_GET_OR_UPDATE)\n    <span class=\"hljs-keyword\">else</span>:\n        <span class=\"hljs-keyword\">return</span> send({<span class=\"hljs-string\">'error'</span> : <span class=\"hljs-string\">'item not found'</span>}, HTTP_NOT_FOUND)\n</code></pre>\n<p>Notice that the above route accepts both <code>collection_name</code> and <code>id</code>. This <code>id</code> is the one which is returned in the Create API.</p>\n<h3 id=\"heading-step-6-update-api\">Step 6: Update API</h3>\n<p>Now, let's create an API to allow updating one entry at a time. It would need to have both the <code>collection_name</code> and <code>id</code> parameters and would be accessible by a <code>PUT</code> request.</p>\n<pre><code class=\"lang-python\"><span class=\"hljs-meta\">@app.route('/&lt;collection_name&gt;/&lt;id&gt;', methods=['PUT'])</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">update_item</span>(<span class=\"hljs-params\">collection_name, id</span>):</span>\n    <span class=\"hljs-string\">\"\"\"\n        Update one item in collection.\n    \"\"\"</span>\n    collection = getattr(mongo.db, collection_name)\n    r = collection.find_one({<span class=\"hljs-string\">'_id'</span>: ObjectId(id)})\n    <span class=\"hljs-keyword\">if</span> r:\n        <span class=\"hljs-keyword\">for</span> key <span class=\"hljs-keyword\">in</span> request.json.keys():\n            r[key] = request.json[key]\n        <span class=\"hljs-keyword\">try</span>:\n            collection.replace_one({<span class=\"hljs-string\">\"_id\"</span>: ObjectId(id)}, r)\n            output = {<span class=\"hljs-string\">'message'</span> : <span class=\"hljs-string\">'item updated'</span>}\n            <span class=\"hljs-keyword\">return</span> send(output, HTTP_SUCCESS_GET_OR_UPDATE)\n        <span class=\"hljs-keyword\">except</span> Exception <span class=\"hljs-keyword\">as</span> e:\n            output = {<span class=\"hljs-string\">'error'</span> : str(e)}\n            <span class=\"hljs-keyword\">return</span> send(output, HTTP_BAD_REQUEST)\n    <span class=\"hljs-keyword\">else</span>:\n        output = {<span class=\"hljs-string\">'error'</span> : <span class=\"hljs-string\">'item not found'</span>}\n        <span class=\"hljs-keyword\">return</span> send(output, HTTP_NOT_FOUND)\n</code></pre>\n<p>Note that before we update the item, we check for its existence. You could skip this behaviour and could create an API which attempts to Create or Update, which is a popular requirement in many use cases.</p>\n<h3 id=\"heading-step-7-delete-api\">Step 7: Delete API</h3>\n<p>A rather simpler API, the Delete API too needs both the <code>collection_name</code> and <code>id</code> and listens to the <code>DELETE</code> request.</p>\n<pre><code class=\"lang-python\"><span class=\"hljs-meta\">@app.route('/&lt;collection_name&gt;/&lt;id&gt;', methods=['DELETE'])</span>\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">delete_item</span>(<span class=\"hljs-params\">collection_name, id</span>):</span>\n    <span class=\"hljs-string\">\"\"\"\n        Delete one item from collection.\n    \"\"\"</span>\n    collection = getattr(mongo.db, collection_name)\n    r = collection.find_one({<span class=\"hljs-string\">'_id'</span>: ObjectId(id)})\n    <span class=\"hljs-keyword\">if</span> r:\n        <span class=\"hljs-keyword\">try</span>:\n            collection.remove(r[<span class=\"hljs-string\">\"_id\"</span>])\n            <span class=\"hljs-keyword\">return</span> send(<span class=\"hljs-string\">\"\"</span>, HTTP_SUCCESS_DELETED)\n        <span class=\"hljs-keyword\">except</span> Exception <span class=\"hljs-keyword\">as</span> e:\n            output = {<span class=\"hljs-string\">'error'</span> : str(e)}\n            <span class=\"hljs-keyword\">return</span> send(output, HTTP_BAD_REQUEST)\n    <span class=\"hljs-keyword\">else</span>:\n        output = {<span class=\"hljs-string\">'error'</span> : <span class=\"hljs-string\">'item not found'</span>}\n        <span class=\"hljs-keyword\">return</span> send(output, HTTP_NOT_FOUND)\n</code></pre>\n<h2 id=\"heading-conclusion\">Conclusion</h2>\n<p>With the above APIs, and any more you might wish to add to it, you can have a simple API wrapper around your MongoDB database which allows you to work with any collection at any moment. To get the full code head over to <a target=\"_blank\" href=\"https://github.com/xprilion/generic-pymongo-flask\">https://github.com/xprilion/generic-pymongo-flask</a>.</p>\n<p>If you read through the code in the <a target=\"_blank\" href=\"http://app.py\"><code>app.py</code></a> on the repository you'll find a few more functions there, which can be a useful addition to your API.</p>\n<p>Thanks for making time for going through this tutorial!</p>\n"},"publishedAt":"2020-09-14T06:30:00.000Z","seo":{"title":"100005","description":null},"tags":[{"slug":"blog"}]}},{"node":{"id":"6571d7e62bce95a6e9bdef2b","slug":"ml-on-2gb-ram","url":"https://xprilion.com/ml-on-2gb-ram","title":"Machine Learning On 2GB RAM","subtitle":null,"brief":"If you think you read the title, or maybe I typed it wrong, you're wrong in both cases. The title proposes an article about performing machine learning with the bare minimum RAM usage, and that's what this article is going to be about.\nYou might thin...","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/stock/unsplash/f57lx37DCM4/upload/7f55d75e935cabc4c4134aef66af1fca.jpeg"},"content":{"html":"<p>If you think you read the title, or maybe I typed it wrong, you're wrong in both cases. The title proposes an article about performing machine learning with the bare minimum RAM usage, and that's what this article is going to be about.</p>\n<p>You might think I am joking. Maybe that's what my life felt like when I did actually survive performing ML on a 2GB RAM laptop. However, I learned much from that phase, and have used those learnings till today to make decisions while choosing the right amount of RAM I would need for running my ML models.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701959639439/0cb8c5bc-0e7b-47b6-bbbd-ae4cbdace665.jpeg\" alt class=\"image--center mx-auto\" /></p>\n<p>I am in no manner an expert of advicing about RAM requirements of applications, neither will I attempt to do so. What I share in this blog are simple observationsal learnings which I believe can be helpful at times.</p>\n<p>Enough prologue, let's dive in!</p>\n<h2 id=\"heading-so-how-did-you-do-it\">So, how did you do it?</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701959528076/58dff192-d286-4108-ba9c-045777195f49.jpeg\" alt class=\"image--center mx-auto\" /></p>\n<p>Okay, it's not magic.</p>\n<p>Rather, I found out all possible ways to reduce the amount of RAM the forever running processes on my laptop were consuming. That left more power for the ML models to run! For this, I had to trim down the list of applications I needed to run parallely on the laptop.</p>\n<p>It can get confusing and tough to choose the 'right set of applications' you need.</p>\n<h2 id=\"heading-how-do-i-decide-what-applications-i-need\">How do I decide what applications I need?</h2>\n<p>If you do not have a definite plan and list of requirements that you have from your system, it can be tough to choose the applications you need to run parallely.  </p>\n<p>In my case, I needed to run machine learning models, but I also had a daily limit on the Internet bandwidth available to me. I had two choices -</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701959588673/cbd42072-c475-4f4e-8c3e-66b9fbfdaad8.jpeg\" alt class=\"image--center mx-auto\" /></p>\n<p>I could choose to run my ML models locally and save on the daily limit on my Internet. But then I would have to spend on buying an extension for the RAM. Or I could run the models online and find a way to minimize my Internet consumption.</p>\n<p>This is a story of how I chose the latter option, and never looked back.</p>\n<blockquote>\n<p>Also, it is a short reminder that decisions made by developers often revolve around financial constraints and that's <strong>okay</strong>.</p>\n</blockquote>\n<p>Once you have laid down all your needs and constraints very clearly before yourself, you'll find it simple to sort things out. Try putting them on a paper! :)</p>\n<h2 id=\"heading-what-applications-did-i-choose\">What applications did I choose?</h2>\n<p>After more decisions like the example above, my list of requirements boiled down to the following -</p>\n<h3 id=\"heading-1-a-minimal-linux-desktop\">1. A minimal Linux desktop</h3>\n<p>For this I chose <a target=\"_blank\" href=\"https://wiki.lxde.org/en/Main_Page\">LXDE</a> which in its default installation contained the <a target=\"_blank\" href=\"https://wiki.lxde.org/en/Openbox\">Openbox</a> desktop environment. It is super minimal, only offering me with a black screen and a menu to get things done in the most frills-away manner.</p>\n<p>With this choice, I immediately dropped the RAM usage by the default applications on my system to something around 200-300 MBs. I still had almost 6x the amount of that RAM to play with!</p>\n<h3 id=\"heading-2-a-code-editor-that-didnt-do-what-i-did-not-ask-it-to\">2. A code editor that didn't do what I did not ask it to</h3>\n<p>While you may love your favorite code editor, it is worthwhile to check the amount of resources it is eating on your system. With <a target=\"_blank\" href=\"https://www.spyder-ide.org/\">Spyder</a>, I felt power, but that power felt sluggish.</p>\n<p>As a replacement to Spyder, I chose to use <a target=\"_blank\" href=\"https://colab.research.google.com/\">Google Colaboratory</a> (we'll talk about this again) in the browser and for non-ML related tasks, I chose <a target=\"_blank\" href=\"https://www.sublimetext.com/3\">Sublime Text</a>.</p>\n<p>Sublime Text gave me a barebones editor with a few cool things, which didn't eat away at the RAM. Hence, leaving me enough free RAM to run a browser!</p>\n<h3 id=\"heading-3-a-browser-that-was-modern-and-fast\">3. A browser that was modern and fast</h3>\n<p>Now this is where you may doubt my choice - I chose <a target=\"_blank\" href=\"https://www.google.com/chrome/\">Google Chrome</a> as my only browser on the system.</p>\n<p>Yes, it did consume a lot of RAM. But at the same time, it offered me superb support for Colaboratory which I was going to use for running my ML models. The decision was simple.</p>\n<p>A habit of minimal Chrome tab usage was developed. I still obsessively close tabs which I do not need.</p>\n<h2 id=\"heading-is-that-all\">Is that all?</h2>\n<p>Yeah, that's pretty much all the setup you'll need to start using your old dusty laptop for machine learning experiments!</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701959507717/42e197d5-ed81-4eca-a5c1-124ec653146d.jpeg\" alt class=\"image--center mx-auto\" /></p>\n<p>Actually, we can have the questions in the comments below.</p>\n<h2 id=\"heading-what-did-you-learn-today\">What did you learn today</h2>\n<p>Today, in under 3 minutes you went through my then painful story of how I started my machine learning exploration on a 2GB RAM machine back in 2016. I hope this inspires you to crunch more results out of your current laptop!</p>\n"},"publishedAt":"2020-04-28T06:30:00.000Z","seo":{"title":"100002","description":null},"tags":[{"slug":"blog"}]}},{"node":{"id":"6571d6c989631b3c87dc448a","slug":"methods-of-integrating-artificial-intelligence-on-flutter","url":"https://xprilion.com/methods-of-integrating-artificial-intelligence-on-flutter","title":"Methods of Integrating Artificial Intelligence on Flutter","subtitle":null,"brief":"One of the coolest UI toolkit's for cross-platform applications, Flutter, a Google offering has grown rapidly in popularity over the last few years. While there are speculations about how Flutter could affect the developer ecosystem, with the under-d...","coverImage":null,"content":{"html":"<hr />\n<p>One of the coolest UI toolkit's for cross-platform applications, <a target=\"_blank\" href=\"https://flutter.dev\">Flutter</a>, a Google offering has grown rapidly in popularity over the last few years. While there are speculations about how Flutter could affect the developer ecosystem, with the under-development <a target=\"_blank\" href=\"https://fuchsia.dev\">Fuchsia</a> constantly making observers thrum with excitement, we cannot ignore the uprising of another popular market term - artificial intelligence (AI). This blog discusses the confluence of Flutter with AI!</p>\n<h2 id=\"heading-but-isnt-ai-super-cool\">But isn't AI super cool?</h2>\n<p>Yes, and it's hot as well!</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701959281248/db5d839c-09d3-4a00-8672-d5fedb536add.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Source: <a target=\"_blank\" href=\"/\">Your Truly</a></p>\n<h2 id=\"heading-should-flutter-devs-care-for-ai\">Should Flutter devs care for AI?</h2>\n<blockquote>\n<p>Definitely.</p>\n</blockquote>\n<p>You'll not be the first to ask why you would need AI on mobile apps, or why would you need a unified way of performing machine learning on your business websites and apps. So let me quickly introduce you to some folks who made it big by incorporating machine learning in their business apps -</p>\n<h3 id=\"heading-1-google\">1. Google</h3>\n<p>When we're talking about machine learning and artificial intelligence in general, Google is one of the foremost names that pop up.</p>\n<p>Being as smart as Google is definitely not an easy task for a human. They leveraged the power of AI as one of the earliest players and today, a plethora of Google products such as GMail, Google Assistant, Google Translate and others.</p>\n<h3 id=\"heading-2-netflix\">2. Netflix</h3>\n<p>Remember how tough it is to stop using the Netflix app once you started to explore the infinite treasures in it. each better than the previous? Netflix was one of the early betters on AI, and today, they lead the market of subscription based video streaming which keeps the viewers glued to their screens due to the power of recommendations that it makes.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701959331263/410cfd75-bf34-4dbc-8981-5d1d201c2b63.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Source: <a target=\"_blank\" href=\"/\">Your Truly Returns</a></p>\n<p>Oh, and, please get your own subscription even if your crush does not ask you for your account. (And tell her to get one too!)</p>\n<hr />\n<p>By now, if you're convinced you do need artificial intelligence on your apps, let's move on to discuss the various methods you have available for making AI enabled apps!</p>\n<h2 id=\"heading-how-do-i-get-ai-on-my-app\">How do I get AI on my app?</h2>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701959350187/fe13d9d2-4e0d-4696-bf50-27b1b5834453.jpeg\" alt class=\"image--center mx-auto\" /></p>\n<p>Source: <a target=\"_blank\" href=\"/\">Your Truly Again</a></p>\n<p>To integrate machine learning with your mobile apps, you\\'ll need a model in place, and then, a way to call that model. In Flutter, the options you have available for running ML models in apps, at the time of writing this blog, are -</p>\n<ul>\n<li><p>Firebase ML Kit</p>\n</li>\n<li><p>On-device models</p>\n</li>\n<li><p>Models as APIs</p>\n</li>\n</ul>\n<p>Let's try to understand what each of these are, and how you would decide which one you would want.</p>\n<h3 id=\"heading-firebase-ml-kit\">Firebase ML Kit</h3>\n<p>ML Kit is a part of the Firebase suite which allows app developers to quickly import and use machine learing models from the Firebase console. Besides having the ability to host and import custom models (not yet available on Firebase for Flutter), there are a number of readymade state-of-the-art models available on Firebase such as -</p>\n<ul>\n<li><p>Face detection</p>\n</li>\n<li><p>On-device Translation</p>\n</li>\n<li><p>Object detection and tracking</p>\n</li>\n<li><p>Smart Reply, and others that keep getting added to the console!</p>\n</li>\n</ul>\n<p>This method is suitable when your application is using Firebase, or you have a custom model but do not want to host it on your own server setup. It is possible to cache the models hosted on Firebase, and hence this method is also suitable for model which expect regular but not very frequent updates.</p>\n<h3 id=\"heading-on-device-models\">On-device models</h3>\n<p>On device models are very powerful if you want to perform high-speed inference directly on the devices of the app users. A very popular way of creating such models is to first create them as TensorFlow models, and then to export them as <code>.tflite</code> files. You can then use the ML Kit plugin in your Flutter app to import from within the project repository the stored <code>.tflite</code> file and run it for inference.</p>\n<p>You can find a sample Flutter application using the Face detection model available on Firebase, by first downloading it and then using it as an on-device model <a target=\"_blank\" href=\"https://github.com/PacktPublishing/Mobile-Deep-Learning-Projects/tree/master/Chapter2/flutter_face_detection\">here</a>.</p>\n<blockquote>\n<p>The code in the sample repository given above has been detailed in <em>Chapter 2: Mobile Vision - Face Detection Using On-Device Models</em> of my book <a target=\"_blank\" href=\"https://www.packtpub.com/data/mobile-deep-learning-projects\">Mobile Deep Learning with TensorFlow Lite, ML Kit and Flutter</a>.</p>\n</blockquote>\n<p>This method is most suitable when you want your app users to experience no lag in the time it takes the model to perform inference, and are also confident that the model will not require rapid updates.</p>\n<h3 id=\"heading-models-as-apis\">Models as APIs</h3>\n<p>This is another very popular method of integrating machine learning with apps. In this method, we wrap the model in an API service and host it using web based servers. These servers could be popular platforms like AWS Lambda, Google App Engine, Heroku or Virtual Machines with a runtime that supports running the model and serving it as a web based service.</p>\n<p>This method is best suitable when you're comfortable with working with servers and web interfaces, and also expect the model to update very frequently.</p>\n<p>I shall be discussing this method in details in the future blogs.</p>\n"},"publishedAt":"2020-04-16T06:30:00.000Z","seo":{"title":"1587052142384","description":null},"tags":[{"slug":"blog"}]}},{"node":{"id":"6571d607743e12b39ae4e7b3","slug":"folding-at-home-setup","url":"https://xprilion.com/folding-at-home-setup","title":"Folding@Home Setup - Local and Cloud VM","subtitle":null,"brief":"The world is facing an unprecedented situation with the Coronavirus outbreak. It is crucial at these times for everyone to join hands and fight the rapidly spreading virus. Computing power available to researchers is very valuable right now. Folding@...","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/stock/unsplash/j2c7yf223Mk/upload/ffba977331ac7b8747b0fc7da3ce1ffb.jpeg"},"content":{"html":"<hr />\n<p>The world is facing an unprecedented situation with the <a target=\"_blank\" href=\"https://www.who.int/emergencies/diseases/novel-coronavirus-2019\">Coronavirus outbreak</a>. It is crucial at these times for everyone to join hands and fight the rapidly spreading virus. Computing power available to researchers is very valuable right now. <a target=\"_blank\" href=\"https://foldingathome.org/\">Folding@Home</a> is a project which allows crowd-sourcing computing power by the help of distributed computing over public compute nodes for the study of biological proteins. The project is currently providing (at the time of writing this article) more than 470 petaFlops of computing power to researchers for studying Coronavirus, and to eventually prepare a cure for it.</p>\n<h2 id=\"heading-but-what-should-i-do\">But, What should I do?</h2>\n<blockquote>\n<p>Contribute.</p>\n</blockquote>\n<p>Your computer's idle time can be utilized towards contributing to the Folding@Home project very, very easily.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701959138349/6b1b1e37-e684-49b1-864a-beef8e55f705.png\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-what-do-i-need\">What do I need?</h2>\n<p>All that you need for contributing to the Folding@Home project -</p>\n<ol>\n<li><p>A working computer, can be local, can be a VM</p>\n</li>\n<li><p>A stable internet connection</p>\n</li>\n<li><p>A will to help, even if for a few hours a day!</p>\n</li>\n</ol>\n<p>Do you have these? If yes (or even if no), read on!</p>\n<h2 id=\"heading-how-to-setup\">How to setup?</h2>\n<p>I shall provide a very quick but complete setup here that should get you started in no time.</p>\n<p>The general procedure is as follows -</p>\n<ol>\n<li><p><a target=\"_blank\" href=\"https://apps.foldingathome.org/passkey/create\">Create a passkey</a> for yourself.</p>\n</li>\n<li><p>Download and install the <code>FAHClient</code> setup on the machine which will perform computation. You'll need your passkey in this step.</p>\n</li>\n<li><p>Download the install the <code>FAHControl</code> setup on the machine which you will use to control and monitor your <code>FAHClient</code>.</p>\n</li>\n<li><p>Any additional configuration as needed.</p>\n</li>\n</ol>\n<p>Let's get started!</p>\n<h3 id=\"heading-setup-on-a-local-machine\">Setup on a local machine</h3>\n<p>Setting up on a local machine is very simple. Follow along the steps below -</p>\n<h4 id=\"heading-windows\">Windows</h4>\n<ol>\n<li><p><a target=\"_blank\" href=\"https://download.foldingathome.org/releases/public/release/fah-installer/windows-10-32bit/v7.5/fah-installer_7.5.1_x86.exe\">Download</a> and run the installer.</p>\n</li>\n<li><p>Click Yes, Next, I Agree, Next, Finish.</p>\n</li>\n<li><p>Enter a name, <code>team number 250120</code> (to join our team, use 0 for no team), and Passkey.</p>\n</li>\n<li><p>Click Save.</p>\n</li>\n</ol>\n<h4 id=\"heading-linux\">Linux</h4>\n<ol>\n<li>Download the <code>FAHClient</code> package for your distro, using the following command:</li>\n</ol>\n<pre><code class=\"lang-bash\"><span class=\"hljs-comment\"># Debian/Ubuntu/Mint</span>\nwget https://download.foldingathome.org/releases/public/release/fahclient/debian-testing-64bit/v7.4/fahclient_7.4.4_amd64.deb\n\n<span class=\"hljs-comment\"># RedHat / CentOS / Fedora</span>\nwget https://download.foldingathome.org/releases/public/release/fahclient/centos-5.3-64bit/v7.4/fahclient-7.4.4-1.x86_64.rpm\n</code></pre>\n<ol>\n<li>Install package using following command:</li>\n</ol>\n<pre><code class=\"lang-bash\"><span class=\"hljs-comment\"># Debian/Ubuntu/Mint</span>\nsudo dpkg -i --force-depends fahclient_7.4.4_amd64.deb\nsudo apt install -f\n\n<span class=\"hljs-comment\"># RedHat / CentOS / Fedora</span>\nsudo rpm -i --nodeps fahclient-7.4.4-1.x86_64.rpm\n</code></pre>\n<ol>\n<li><p>Enter a name, <code>team number 250120</code> (to join our team, use 0 for no team), and Passkey.</p>\n</li>\n<li><p>Complete the <code>FAHClient</code> installation.</p>\n</li>\n<li><p>Download <code>FAHControl</code> using the following command:</p>\n</li>\n</ol>\n<pre><code class=\"lang-bash\"><span class=\"hljs-comment\"># Debian/Ubuntu/Mint</span>\nwget https://download.foldingathome.org/releases/public/release/fahcontrol/debian-testing-64bit/v7.4/fahcontrol_7.4.4-1_all.deb\n\n<span class=\"hljs-comment\"># RedHat / CentOS / Fedora</span>\nwget https://download.foldingathome.org/releases/public/release/fahcontrol/centos-5.3-64bit/v7.4/fahcontrol-7.4.4-1.noarch.rpm\n</code></pre>\n<ol>\n<li>Install FAHControl using the following command:</li>\n</ol>\n<pre><code class=\"lang-bash\"><span class=\"hljs-comment\"># Debian/Ubuntu/Mint</span>\nsudo dpkg -i --force-depends fahcontrol_7.4.4-1_all.deb\nsudo apt install -f\n\n<span class=\"hljs-comment\"># RedHat / CentOS / Fedora</span>\nsudo rpm -i --nodeps fahcontrol-7.4.4-1.noarch.rpm\n</code></pre>\n<h3 id=\"heading-setup-on-a-vm\">Setup on a VM</h3>\n<p>After provisioning your VM, assuming it is a Linux VM (we prefer Ubuntu 18.04 LTS), follow the given steps -</p>\n<h4 id=\"heading-steps-to-be-performed-on-vm\">Steps to be performed on VM</h4>\n<ol>\n<li>Update your package repository.</li>\n</ol>\n<pre><code class=\"lang-bash\">sudo apt update\n</code></pre>\n<ol>\n<li>Download the <code>FAHClient</code> package for your distro, using the following command:</li>\n</ol>\n<pre><code class=\"lang-bash\"><span class=\"hljs-comment\"># Debian/Ubuntu/Mint</span>\nwget https://download.foldingathome.org/releases/public/release/fahclient/debian-testing-64bit/v7.4/fahclient_7.4.4_amd64.deb\n</code></pre>\n<ol>\n<li>Install package using the following command:</li>\n</ol>\n<pre><code class=\"lang-plaintext\"># Debian/Ubuntu/Mint\nsudo dpkg -i --force-depends fahclient_7.4.4_amd64.deb\nsudo apt install -f\n</code></pre>\n<ol>\n<li><p>Enter a name, <code>team number 250120</code> (to join our team, use 0 for no team), and Passkey.</p>\n</li>\n<li><p>Complete the <code>FAHClient</code> installation.</p>\n</li>\n<li><p>Stop the <code>FAHClient</code></p>\n</li>\n</ol>\n<pre><code class=\"lang-bash\">sudo /etc/init.d/FAHClient stop\n</code></pre>\n<ol>\n<li>Edit the configuration file, as shown below:</li>\n</ol>\n<pre><code class=\"lang-bash\">sudo nano /etc/fahclient/config.xml\n</code></pre>\n<p>Your file should look like:</p>\n<pre><code class=\"lang-text\">&lt;config&gt;\n  &lt;!-- Client Control --&gt;\n  &lt;fold-anon v='true'/&gt;\n\n  &lt;!-- Folding Slot Configuration --&gt;\n  &lt;gpu v='false'/&gt;\n\n  &lt;!-- HTTP Server --&gt;\n  &lt;allow v='127.0.0.1 your_local_system_ip'/&gt;\n\n  &lt;!-- Slot Control --&gt;\n  &lt;power v='full'/&gt;\n\n  &lt;!-- User Information --&gt;\n  &lt;passkey v='your_passkey'/&gt;\n  &lt;team v='250120'/&gt;\n  &lt;user v='your_name'/&gt;\n\n  &lt;!-- Web Server --&gt;\n  &lt;web-allow v='127.0.0.1 your_local_system_ip'/&gt;\n\n  &lt;!-- Folding Slots --&gt;\n  &lt;slot id='0' type='CPU'/&gt;\n&lt;/config&gt;\n</code></pre>\n<ol>\n<li>Restart the <code>FAHClient</code></li>\n</ol>\n<pre><code class=\"lang-bash\">sudo /etc/init.d/FAHClient start\n</code></pre>\n<h4 id=\"heading-steps-to-be-performed-locally\">Steps to be performed locally</h4>\n<ol>\n<li><p>Now, you'll need to install the <code>FAHControl</code> on your local system. Use the steps given in the <a class=\"post-section-overview\" href=\"#Setup-on-a-local-machine\">Setup on local machine</a> section to install <code>FAHControl</code> on your local system.</p>\n</li>\n<li><p>You should be able to access your remote <code>FAHClient</code> using the following address:</p>\n</li>\n</ol>\n<pre><code class=\"lang-bash\">http://your_vm_ip:7396\n</code></pre>\n<p>You should see the dashboard displaying information as shown in the following screenshot:</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701959155321/c6a11ff0-ac6c-4c94-b1c4-5496ba14283b.png\" alt class=\"image--center mx-auto\" /></p>\n<p>You can always head over the <a target=\"_blank\" href=\"https://foldingathome.org/support/faq/\">official installation documentation</a> for more details and options! You can also ask questions on the <a target=\"_blank\" href=\"https://foldingforum.org/\">Folding Forum</a>.</p>\n"},"publishedAt":"2020-03-24T06:30:00.000Z","seo":{"title":"100006","description":null},"tags":[{"slug":"blog"}]}},{"node":{"id":"6571bbf8fa8bce85e8889f24","slug":"being-noob","url":"https://xprilion.com/being-noob","title":"Being Noob - Starting a tech journey","subtitle":null,"brief":"Hi there! I realized I began sort of suddenly, talking about networks first and then a tech scholarship, while not having yet talked about how things began for me and what I believe is a good way to launch a journey in computer science.\nSo, talking a...","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/stock/unsplash/OyCl7Y4y0Bk/upload/74a58e2c97cbc8e811abecb917ab2270.jpeg"},"content":{"html":"<p>Hi there! I realized I began sort of suddenly, talking about networks first and then a tech scholarship, while not having yet talked about how things began for me and what I believe is a good way to launch a journey in computer science.</p>\n<h2 id=\"heading-so-talking-about-past-eh\">So, talking about past, eh?</h2>\n<p>No really, but sort of, but unkinda.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701952620785/adca4f1b-e221-4a2b-ba65-0a0ed4b25583.jpeg\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-how-does-it-all-begin\">How does it all begin?</h2>\n<pre><code class=\"lang-python\"><span class=\"hljs-keyword\">print</span> <span class=\"hljs-string\">\"Hello World\"</span>\n</code></pre>\n<p>Starting a journey in tech is more about willing to explore the unknown. Unlike other fields, its not about reading up a good deal of literature first and then trying to apply it.</p>\n<blockquote>\n<p>No programmer reads the documentation beyond Getting Started unless they're stuck.</p>\n</blockquote>\n<p>And that's what you should do with your journey - if you really wish to start moving forward in your tech journey, stop trying to know <em>all</em> about it. Just be like this -</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701952639766/5fbcb65a-eb5e-41a5-97e3-d1b9910799f7.jpeg\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-oh-but-what-do-i-do\">Oh but what do I do?</h2>\n<p>What you do once you're started is simple - you do what you love doing. Computer Science is that one field which holds the potential to help you in whatever passion you hold - from high school algebra to politics (<a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/The_Great_Hack\">The Great Hack</a>) or discovering new planets, your tech journey can take you pretty much anywhere you wish to go!</p>\n<p>A nice way to begin climbing the mountain is to identify it. Set goals of what you wish to build.</p>\n<blockquote>\n<p>Don't learn a language just because you wish to speak it, but don't have anything to say.</p>\n</blockquote>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701952656254/2fff90b9-6943-4b4c-8a35-05d600b4dae9.jpeg\" alt class=\"image--center mx-auto\" /></p>\n<h2 id=\"heading-buthow-do-i-learn-x-and-y-to-do-z\">But..how do I learn X and Y to do Z?</h2>\n<blockquote>\n<p>Google.</p>\n</blockquote>\n<p>Reading blogs, documentations, papers is probably the best habit you can develop in your tech journey. Start being a regular at Google and begin searching out all possible ways of expressing your query. Read as many resources you can get. Don't straight off jump into video courses and lectures - take time to understand from blogs what you're trying to learn and how it is going to be useful to you.</p>\n<blockquote>\n<p>Don't be technology intolerant.</p>\n</blockquote>\n<p>Be fair to all frameworks or libraries you know or do not know of when deciding what you need to learn to build something. Don't jump into learning something just because others are doing it. Remember, thoughts come before words, and we choose words based on thoughts. Choose your frameworks or languages based on what best suits the product you wish to build.</p>\n<h2 id=\"heading-what-did-you-learn-today\">What did you learn today</h2>\n<p>Today, in under 3 minutes you went through my take on what are the most important points you need to remember when starting a journey in tech!</p>\n<p>I'll post more on this blog on 16th Jan, 2020! Bookmark my site to stay tuned!</p>\n<h3 id=\"heading-about-me\">About me</h3>\n<p><em>I am a web developer turned machine learning explorer who loves mixing and matching various stacks! I lead the team at</em> <a target=\"_blank\" href=\"https://github.com/thecodefoundation\"><em>The Code Foundation</em></a> <em>and we love solving problems about computer vision and NLP!</em></p>\n<p><em>Send me a hi on</em> <a target=\"_blank\" href=\"mailto:hi@xpri.dev\"><em>hi@xpri.dev</em></a></p>\n"},"publishedAt":"2020-01-12T06:30:00.000Z","seo":{"title":"100008","description":null},"tags":[{"slug":"blog"}]}},{"node":{"id":"6571d27119dcb2af76d1a9f3","slug":"venkat-panchapakesan-memorial-scholarship-part-1","url":"https://xprilion.com/venkat-panchapakesan-memorial-scholarship-part-1","title":"What is the Venkat Panchapakesan Memorial Scholarship and Why its not won in a day","subtitle":null,"brief":"It was with a crazy smile and immense gratitude that I read through the entire mail from Google India about being awarded the Venkat Panchapakesan Memorial Scholarship for the session 2019-20. An amazing and prestigious scholarship, I am really happy...","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/stock/unsplash/Sj0iMtq_Z4w/upload/51a402ef957c499f9e47f4554f53b819.jpeg"},"content":{"html":"<p>It was with a crazy smile and immense gratitude that I read through the entire mail from Google India about being awarded the <a target=\"_blank\" href=\"https://buildyourfuture.withgoogle.com/scholarships/venkat-panchapakesan-memorial-scholarship/\">Venkat Panchapakesan Memorial Scholarship</a> for the session 2019-20. An amazing and prestigious scholarship, I am really happy to have been made a part of it and would love to talk more about it in hope that it would help someone deserving receive the same in the future.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701958188504/62a13dd2-2385-4369-ad5a-431824e3fd4d.png\" alt class=\"image--center mx-auto\" /></p>\n<p>So,</p>\n<h2 id=\"heading-what-is-venkat-panchapakesan-memorial-scholarship-all-about\">What is Venkat Panchapakesan Memorial Scholarship all about?</h2>\n<p>The answer to this question begins with another question -</p>\n<h3 id=\"heading-who-was-venkat-panchapakesan\">Who was Venkat Panchapakesan?</h3>\n<p>A description from <a target=\"_blank\" href=\"https://buildyourfuture.withgoogle.com/scholarships/venkat-panchapakesan-memorial-scholarship/\">Google's Web page about the scholarship</a> describes Venkat as given below -</p>\n<pre><code class=\"lang-plaintext\">Venkat was a much loved and highly respected engineer whose career took him to YouTube, Google, and Yahoo, among other notable companies. He tragically passed away too young, after a battle with cancer. During his short time he deeply touched the hearts and minds of his friends, family and colleagues. He taught us to be generous, humble, ever-optimistic and to always find the best in people.\n</code></pre>\n<p>Once you read through, and try taking a look at some videos on YouTube about Venkat, you'll come to realize the beauty of his personality and what made him so loved. But above that, you'll realize what it means to be compassionate, supporting and forever happy to help. It is this understanding, which will help you answer the following question.</p>\n<h3 id=\"heading-who-is-an-ideal-candidate-for-winning-this-scholarship\">Who is an ideal candidate for winning this scholarship?</h3>\n<p>Frankly, that is best known to the selectors of the scholarship. In my opinion, my profile stood out due to my contribution to the various tech communities around me and my willingness to use technology for achieveing sustainable development goals. Hence, someone who loves helping people, goes beyond their way to aid those around them in their problems, loves spreading the word about technology and believes that they can help the world become a better and sustainable place with the help of technology would probably stand a good chance at winning the scholarship!</p>\n<p>As long as you're leaving a positive impact on even a handful of people around you, you're doing good!</p>\n<h3 id=\"heading-and-do-i-need-to-be-a-super-programmer\">And do I need to be a super programmer?</h3>\n<p>No! All you need to do is to know how to and be willing to use technology for making this world a more sustainable place and to help people around you in having a better life. Now, 'sustainable' could have a number of meanings, you can Google up <code>United Nations Sustainable Development Goals</code> and study about the several factors which contribute towards making the world a better place to live in! Remember, at the end of the day, the best person is he/she who uses whatever they have learnt for the best of people around them, in whatever measure they are able to!</p>\n<h2 id=\"heading-this-scholarship-is-not-won-in-a-day\">This scholarship is not won in a day :)</h2>\n<p>From the above discussion so far, you'd have realized that winning this scholarship may require months or even years of previous work. Hence, to maximize your chances of being among the lucky 5-6 next year who bag this amazing opportunity, make sure you start giving back to your local ecosystem in any manner that's possible for you! It will take time, patience, effort but at the end of it all, regarldess you win it or not, you'll find yourself a much better version of yourself and will definitely be a loved face among the people around you!</p>\n<h2 id=\"heading-what-did-you-learn-today\">What did you learn today</h2>\n<p>Today, in under 3 minutes you learnt about the Venkat Panchapakesan Memorial Scholarship and why I think this scholarship recognizes those who do their best over a long period of time to help people around them using technology. There will be more posts coming on this topic in the future at regular intervals, and I hope you'll be able to make the best of them when applying in the future!</p>\n"},"publishedAt":"2020-01-08T06:30:00.000Z","seo":{"title":"1578482711512","description":null},"tags":[{"slug":"blog"}]}},{"node":{"id":"6571d1d67cbd3da3fa811d72","slug":"difference-between-localhost-127001-and-0000","url":"https://xprilion.com/difference-between-localhost-127001-and-0000","title":"Difference between localhost, 127.0.0.1 and 0.0.0.0","subtitle":null,"brief":"I've been often asked, why do we sometimes need to specify the IP for a program to be 0.0.0.0 in order to get it to be accessible from machines outsides the network. I shall attempt to answer the question in this post.\nWhat is 127.0.0.1\n127.0.0.1 is ...","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/stock/unsplash/40XgDxBfYXM/upload/68f344b07cb3326e732d0e828a050756.jpeg"},"content":{"html":"<p>I've been often asked, why do we sometimes need to specify the IP for a program to be <code>0.0.0.0</code> in order to get it to be accessible from machines outsides the network. I shall attempt to answer the question in this post.</p>\n<h2 id=\"heading-what-is-127001\">What is 127.0.0.1</h2>\n<p><code>127.0.0.1</code> is the loopback address. It means, whatever request is made, send it back to the same machine. When a remote host calls this, they're actually calling themselves.</p>\n<h2 id=\"heading-what-is-localhosthttplocalhost\">What is <a target=\"_blank\" href=\"http://localhost\">localhost</a></h2>\n<p><a target=\"_blank\" href=\"http://localhost\"><code>localhost</code></a> is simply a conventional mapping done in the <code>hosts</code> file of the OS. It can be changed. I could ask <a target=\"_blank\" href=\"http://google.com\"><code>google.com</code></a> to point to my loopback address and that would work same.</p>\n<h2 id=\"heading-what-is-0000\">What is 0.0.0.0</h2>\n<p><code>0.0.0.0</code> is a wildcard for all open routes to the system within the system where the script runs.</p>\n<h2 id=\"heading-when-do-you-face-the-need-to-differentiate-between-them\">When do you face the need to differentiate between them -</h2>\n<p>We specifically face the problem of differentiating them in Flask/Django/NodeJS applications because these are runtimes which require the user to define which IP (or routes) to listen on. If you've ever tried web development on PHP, you'd realize that the <a target=\"_blank\" href=\"http://localhost\"><code>localhost</code></a> web server was accessible using the IP of the computer's LAN as well from other devices (say your mobile). This is because by default Apache listens on <code>*:80</code> which essentially means <code>0.0.0.0:80</code>, as given in its default virtual host configuration file -</p>\n<pre><code class=\"lang-markdown\"><span class=\"hljs-section\"># Ensure that Apache listens on port 80</span>\nListen 80\n<span class=\"xml\"><span class=\"hljs-tag\">&lt;<span class=\"hljs-name\">VirtualHost</span> *<span class=\"hljs-attr\">:80</span>&gt;</span></span>  # Notice this\n<span class=\"hljs-code\">    DocumentRoot \"/var/www/html\" # OR \"C:/xampp/htdocs\" for Windows systems with XAMPP installation\n</span>\n<span class=\"hljs-code\">    # Other directives here\n&lt;/VirtualHost&gt;</span>\n</code></pre>\n"},"publishedAt":"2020-01-04T06:30:00.000Z","seo":{"title":"100007","description":null},"tags":[{"slug":"blog"}]}},{"node":{"id":"6571d0f614b1d7b96c3eb6bb","slug":"hello-world","url":"https://xprilion.com/hello-world","title":"OMG, a new look!","subtitle":null,"brief":"Hey there! If you have visited my website in 2019 or before, you would know that my website was pretty blue and very blue. Overall, it was unexciting, and there was no fresh content on it. But did that worry me? Oh yes.\nSo what did I do about it?\nDit...","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/stock/unsplash/mG28olYFgHI/upload/cd397abfa7c2cf146da8ae9af0297a41.jpeg"},"content":{"html":"<p>Hey there! If you have visited my website in 2019 or before, you would know that my website was pretty blue and very blue. Overall, it was unexciting, and there was no fresh content on it. But did that worry me? Oh yes.</p>\n<h2 id=\"heading-so-what-did-i-do-about-it\">So what did I do about it?</h2>\n<h3 id=\"heading-ditch-php-website\">Ditch PHP website</h3>\n<pre><code class=\"lang-bash\">$ <span class=\"hljs-built_in\">echo</span> <span class=\"hljs-string\">\"I are poor men\"</span>\n</code></pre>\n<p>So I ditched my legacy website (constant since 2016) and began the hunt of a perfect way to put forward my work and to have more fun on this website instead of simply hosting my unupdated resume.</p>\n<p>Static Portfolios are BORING:</p>\n<p><img src=\"https://media3.giphy.com/media/l0HlzgveB7SAZC6Iw/giphy.gif\" alt=\"Anubhav Singh Says - Static Portfolios are boring\" /></p>\n<p>But PHP websites cost. And I needed to fit this thing in my budget.</p>\n<h2 id=\"heading-hoping-for-more-content-in-future-d\">Hoping for more content in future :D</h2>\n<pre><code class=\"lang-bash\">$ Anubhav <span class=\"hljs-string\">\"write\"</span>.\n</code></pre>\n<p>I have this bad habit of creating cool blog websites, and forgetting to write on them. This time, I do hope I am able to keep writing stuff here.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1701957867941/424d770d-fca3-4fc3-ad78-ef2d82d3f741.jpeg\" alt class=\"image--center mx-auto\" /></p>\n<p>Only hoping that I shall be able to keep this blog alive!</p>\n<p>Thanks for hanging around! :)</p>\n"},"publishedAt":"2019-11-24T06:30:00.000Z","seo":{"title":"100004","description":null},"tags":[{"slug":"blog"}]}}]}},"staticQueryHashes":[],"slicesMap":{}}