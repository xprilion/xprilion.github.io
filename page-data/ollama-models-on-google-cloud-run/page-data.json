{"componentChunkName":"component---src-templates-blog-post-tsx","path":"/ollama-models-on-google-cloud-run/","result":{"pageContext":{"id":"667dabf5c02c8a29c0b8ca61","node":{"id":"667dabf5c02c8a29c0b8ca61","slug":"ollama-models-on-google-cloud-run","url":"https://xprilion.com/ollama-models-on-google-cloud-run","title":"Ollama Models on Cloud Run","featured":false,"subtitle":null,"brief":"This blog is a read-along for the repository xprilion/ollama-cloud-run which shows how to deploy various models using the Ollama API on Cloud Run, to run inference using CPU only on a serverless platform - incurring bills only when you use them.\nOlla...","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1719511984949/a514362c-99a9-4aad-9678-c012e0c85eed.png"},"content":{"html":"<p>This blog is a read-along for the repository <a target=\"_blank\" href=\"https://github.com/xprilion/ollama-cloud-run/\">xprilion/ollama-cloud-run</a> which shows how to deploy various models using the Ollama API on Cloud Run, to run inference using CPU only on a serverless platform - incurring bills only when you use them.</p>\n<p><strong>Ollama</strong> is a framework that makes it easy for developers to prototype apps with open models. It comes with a REST API, and this repository provides Dockerfiles and deployment scripts for each model.</p>\n<p><strong>Google Cloud Run</strong> is a fully managed compute platform that automatically scales your stateless containers. You can run code in any language, and all dependencies are included in a container image, which Google Cloud Run handles deployment and scaling for.</p>\n<p>Ispiration (and gemma2b code) from <a target=\"_blank\" href=\"https://github.com/wietsevenema/samples/tree/main/run/ollama-gemma\">wietsevenema/samples</a>.</p>\n<h2 id=\"heading-models\">Models</h2>\n<div class=\"hn-table\">\n<table>\n<thead>\n<tr>\n<td>Model</td><td>Version</td><td>Folder Link</td></tr>\n</thead>\n<tbody>\n<tr>\n<td>codegemma</td><td>2b</td><td><a target=\"_blank\" href=\"https://github.com/xprilion/ollama-cloud-run/tree/main/codegemma/2b\">codegemma/2b</a></td></tr>\n<tr>\n<td>codegemma</td><td>7b</td><td><a target=\"_blank\" href=\"https://github.com/xprilion/ollama-cloud-run/tree/main/codegemma/7b\">codegemma/7b</a></td></tr>\n<tr>\n<td>gemma</td><td>2b</td><td><a target=\"_blank\" href=\"https://github.com/xprilion/ollama-cloud-run/tree/main/gemma/2b\">gemma/2b</a></td></tr>\n<tr>\n<td>gemma</td><td>7b</td><td><a target=\"_blank\" href=\"https://github.com/xprilion/ollama-cloud-run/tree/main/gemma/7b\">gemma/7b</a></td></tr>\n<tr>\n<td>gemma2</td><td>9b</td><td><a target=\"_blank\" href=\"https://github.com/xprilion/ollama-cloud-run/tree/main/gemma2/9b\">gemma2/9b</a></td></tr>\n<tr>\n<td>llama3</td><td>8b</td><td><a target=\"_blank\" href=\"https://github.com/xprilion/ollama-cloud-run/tree/main/llama3/8b\">llama3/8b</a></td></tr>\n<tr>\n<td>llava</td><td>7b</td><td><a target=\"_blank\" href=\"https://github.com/xprilion/ollama-cloud-run/tree/main/llava/7b\">llava/7b</a></td></tr>\n<tr>\n<td>mistral</td><td>7b</td><td><a target=\"_blank\" href=\"https://github.com/xprilion/ollama-cloud-run/tree/main/mistral/7b\">mistral/7b</a></td></tr>\n<tr>\n<td>phi3</td><td>3.8b</td><td><a target=\"_blank\" href=\"https://github.com/xprilion/ollama-cloud-run/tree/main/phi3/3.8b\">phi3/3.8b</a></td></tr>\n<tr>\n<td>qwen2</td><td>0.5b</td><td><a target=\"_blank\" href=\"https://github.com/xprilion/ollama-cloud-run/tree/main/qwen2/0.5b\">qwen2/0.5b</a></td></tr>\n</tbody>\n</table>\n</div><h2 id=\"heading-usage\">Usage</h2>\n<p>To build the container with a specific model included and deploy the Ollama API to a publicly accessible URL on Cloud Run, use the following command from the corresponding model's directory. For example, to deploy <code>gemma:2b</code>:</p>\n<pre><code class=\"lang-sh\">bash gemma/2b/deploy.sh\n</code></pre>\n<p>Respond to any prompts the command gives you. You might need to enable a few APIs and choose a region to deploy to.</p>\n<p>Building the container takes roughly 3-20 minutes, depending on model size.</p>\n<p>Once the command completes, the deploy command shows the public URL of the service.</p>\n<h2 id=\"heading-explore-the-api\">Explore the API</h2>\n<p>Ask the deployed model a question:</p>\n<pre><code class=\"lang-sh\">curl &lt;PUBLIC_URL&gt;/api/generate -d \\\n <span class=\"hljs-string\">'{ \n    \"model\": \"gemma:2b\", \n    \"prompt\": \"Why is the sky blue?\" \n  }'</span>\n</code></pre>\n<p>The first request to a new instance will take some extra setup time because the model is loaded into memory. Ollama keeps the model in memory for 5 minutes.</p>\n<p>For the full Ollama API, refer to <a target=\"_blank\" href=\"https://github.com/ollama/ollama/blob/main/docs/api.md\">the API docs</a>.</p>\n<h2 id=\"heading-clean-up\">Clean Up</h2>\n<p>To clean up after following this short tutorial, you can do the following:</p>\n<ul>\n<li><p>In Artifact Registry, find the <code>cloud-run-source-deploy</code> repository and remove the container image used by the Cloud Run service you created.</p>\n</li>\n<li><p>In Cloud Run, delete the service you created.</p>\n</li>\n</ul>\n<h2 id=\"heading-links\">Links</h2>\n<ul>\n<li><a target=\"_blank\" href=\"https://github.com/ollama/ollama\">Ollama</a></li>\n</ul>\n<hr />\n<p>Use for research, exploration, and prototyping.</p>\n"},"publishedAt":"2024-06-27T18:14:13.801Z","seo":{"title":null,"description":null},"tags":[{"slug":"google"},{"slug":"cloud"},{"slug":"cloudrun"},{"slug":"ollama"},{"slug":"llm"},{"slug":"serverless"},{"slug":"deployment"},{"slug":"blog"}]},"ogImageUrl":"https://cdn.xpri.dev/covers/667dabf5c02c8a29c0b8ca61.png"}},"staticQueryHashes":[],"slicesMap":{}}